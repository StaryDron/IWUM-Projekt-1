{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# KONFIG\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))   # katalog z tym plikiem\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "\n",
    "# EDA_wyniki w BASE (np. .../EDA/EDA_wyniki)\n",
    "SAVE_DIR = os.path.join(BASE_DIR, \"EDA_wyniki\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# WCZYTANIE DANYCH\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"\\n=== Podstawowe informacje ===\")\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# ===================================================================\n",
    "# 1) BAR PLOT: liczba default√≥w\n",
    "# ===================================================================\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df[\"default\"].value_counts().sort_index().plot(kind=\"bar\", color=[\"green\", \"red\"])\n",
    "plt.title(\"Rozk≈Çad default√≥w\")\n",
    "plt.xlabel(\"Default (0 = good, 1 = bad)\")\n",
    "plt.ylabel(\"Liczba obserwacji\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"default_distribution.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ===================================================================\n",
    "# 2) Podzia≈Ç train/val/test + bar plot\n",
    "# ===================================================================\n",
    "\n",
    "X = df.drop(columns=[\"default\"])\n",
    "y = df[\"default\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "sizes = {\n",
    "    \"Train\": len(X_train),\n",
    "    \"Validation\": len(X_val),\n",
    "    \"Test\": len(X_test),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(sizes.keys(), sizes.values(), color=[\"blue\", \"orange\", \"green\"])\n",
    "plt.title(\"Liczno≈õƒá zbior√≥w: train / val / test\")\n",
    "plt.ylabel(\"Liczba obserwacji\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"train_val_test_counts.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ===================================================================\n",
    "# 3) Braki danych\n",
    "# ===================================================================\n",
    "\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "print(\"\\n=== Kolumny z brakami ===\")\n",
    "print(missing)\n",
    "\n",
    "missing.to_csv(os.path.join(SAVE_DIR, \"missing_values.csv\"))\n",
    "\n",
    "# ===================================================================\n",
    "# 4) Korelacje ‚Äì heatmap dla 10 zmiennych NAJBARDZIEJ skorelowanych z defaultem\n",
    "# ===================================================================\n",
    "\n",
    "# bierzemy tylko kolumny numeryczne\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c != \"default\"]\n",
    "\n",
    "# korelacje z defaultem\n",
    "corr_full = df[numeric_cols + [\"default\"]].corr()\n",
    "corr_with_target = corr_full[\"default\"].drop(\"default\")\n",
    "\n",
    "top10_vars = corr_with_target.abs().sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "print(\"\\nTop 10 zmiennych najbardziej skorelowanych z defaultem:\")\n",
    "print(top10_vars)\n",
    "\n",
    "corr_top10 = df[top10_vars + [\"default\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_top10, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Heatmap korelacji ‚Äì top 10 zmiennych vs default\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"correlation_heatmap_top10.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ===================================================================\n",
    "# 5) Scatter ploty dla najmocniejszych zmiennych\n",
    "#     ‚Äì wybieramy 3 najmocniej skorelowane z defaultem\n",
    "# ===================================================================\n",
    "\n",
    "target_corr_top = corr_top10[\"default\"].abs().sort_values(ascending=False)\n",
    "strong_vars = target_corr_top.index[1:4]  # pomijamy 'default' na pozycji 0\n",
    "\n",
    "print(\"\\nNajsilniej skorelowane zmienne z defaultem (do scatter√≥w):\")\n",
    "print(strong_vars)\n",
    "\n",
    "# Parowe scatter ploty\n",
    "for i in range(len(strong_vars)):\n",
    "    for j in range(i + 1, len(strong_vars)):\n",
    "        v1 = strong_vars[i]\n",
    "        v2 = strong_vars[j]\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.scatterplot(\n",
    "            data=df,\n",
    "            x=v1,\n",
    "            y=v2,\n",
    "            hue=\"default\",\n",
    "            palette={0: \"green\", 1: \"red\"},\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        plt.title(f\"Scatter: {v1} vs {v2} (kolor=default)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_DIR, f\"scatter_{v1}_{v2}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\nEDA zapisane do folderu:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_transformers.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InfinityReplacer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Zamienia inf/-inf na NaN.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech - bez zmian.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array(input_features)\n",
    "\n",
    "\n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Usuwa kolumny z liczbƒÖ brak√≥w przekraczajƒÖcƒÖ threshold.\"\"\"\n",
    "\n",
    "    def __init__(self, missing_threshold=0.95):\n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        missing_ratio = X.isnull().mean()\n",
    "        self.cols_to_drop_ = missing_ratio[missing_ratio > self.missing_threshold].index.tolist()\n",
    "        if len(self.cols_to_drop_) > 0:\n",
    "            print(f\"üóëÔ∏è Zapamiƒôtano {len(self.cols_to_drop_)} kolumn do usuniƒôcia (braki > {self.missing_threshold*100:.0f}%)\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_, errors='ignore')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po usuniƒôciu kolumn.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array([col for col in input_features if col not in self.cols_to_drop_])\n",
    "\n",
    "\n",
    "class MissingIndicator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Dodaje flagi *_missing dla kolumn z brakami.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cols_with_missing_ = X.columns[X.isnull().any()].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.cols_with_missing_:\n",
    "            if col in X.columns:\n",
    "                X[f\"{col}_missing\"] = X[col].isnull().astype(int)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech + flagi _missing.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        output_cols = list(input_features)\n",
    "        for col in self.cols_with_missing_:\n",
    "            if col in input_features:\n",
    "                output_cols.append(f\"{col}_missing\")\n",
    "        return np.array(output_cols)\n",
    "\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Imputacja: numeryczne -> mediana, kategoryczne -> moda.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.imputer_num_ = None\n",
    "        self.imputer_cat_ = None\n",
    "        self.num_cols_ = None\n",
    "        self.cat_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        bool_cols = X.select_dtypes(include=[bool]).columns.tolist()\n",
    "        self.num_cols_ = [col for col in self.num_cols_ if col not in bool_cols]\n",
    "\n",
    "        self.cat_cols_ = X.select_dtypes(exclude=[np.number, np.bool_]).columns.tolist()\n",
    "\n",
    "        if len(self.num_cols_) > 0:\n",
    "            self.imputer_num_ = SimpleImputer(strategy=\"median\")\n",
    "            self.imputer_num_.fit(X[self.num_cols_])\n",
    "\n",
    "        if len(self.cat_cols_) > 0:\n",
    "            self.imputer_cat_ = SimpleImputer(strategy=\"most_frequent\")\n",
    "            self.imputer_cat_.fit(X[self.cat_cols_])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.imputer_num_ is not None and len(self.num_cols_) > 0:\n",
    "            X[self.num_cols_] = self.imputer_num_.transform(X[self.num_cols_])\n",
    "\n",
    "        if self.imputer_cat_ is not None and len(self.cat_cols_) > 0:\n",
    "            X[self.cat_cols_] = self.imputer_cat_.transform(X[self.cat_cols_])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech - bez zmian.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array(input_features)\n",
    "\n",
    "\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Winsoryzacja (obcina warto≈õci skrajne na podstawie kwantyli).\"\"\"\n",
    "\n",
    "    def __init__(self, lower_q=0.02, upper_q=0.98):\n",
    "        self.lower_q = lower_q\n",
    "        self.upper_q = upper_q\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        bool_cols = X.select_dtypes(include=[bool]).columns\n",
    "        num_cols = [col for col in num_cols \n",
    "                    if col not in bool_cols and not col.endswith(\"_missing\")]\n",
    "\n",
    "        self.limits_ = {}\n",
    "        for col in num_cols:\n",
    "            lower = X[col].quantile(self.lower_q)\n",
    "            upper = X[col].quantile(self.upper_q)\n",
    "            self.limits_[col] = (lower, upper)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col, (lower, upper) in self.limits_.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = np.clip(X[col], lower, upper)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech - bez zmian.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array(input_features)\n",
    "\n",
    "\n",
    "class NumericScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Standaryzacja kolumn numerycznych (pomija bool i *_missing).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler_ = None\n",
    "        self.num_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        bool_cols = X.select_dtypes(include=[bool]).columns.tolist()\n",
    "        self.num_cols_ = [col for col in num_cols \n",
    "                         if col not in bool_cols and not col.endswith(\"_missing\")]\n",
    "\n",
    "        if len(self.num_cols_) > 0:\n",
    "            self.scaler_ = StandardScaler()\n",
    "            self.scaler_.fit(X[self.num_cols_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.scaler_ is not None and len(self.num_cols_) > 0:\n",
    "            X[self.num_cols_] = self.scaler_.transform(X[self.num_cols_])\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech - bez zmian.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array(input_features)\n",
    "\n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"One-hot encoding dla kolumn kategorycznych.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cat_cols_ = None\n",
    "        self.encoded_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cat_cols_ = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        if len(self.cat_cols_) > 0:\n",
    "            X_encoded = pd.get_dummies(X, columns=self.cat_cols_, prefix=self.cat_cols_)\n",
    "            self.encoded_cols_ = X_encoded.columns.tolist()\n",
    "        else:\n",
    "            self.encoded_cols_ = X.columns.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if len(self.cat_cols_) > 0:\n",
    "            X = pd.get_dummies(X, columns=self.cat_cols_, prefix=self.cat_cols_)\n",
    "            for col in self.encoded_cols_:\n",
    "                if col not in X.columns:\n",
    "                    X[col] = 0\n",
    "            X = X[self.encoded_cols_]\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po one-hot encoding.\"\"\"\n",
    "        return np.array(self.encoded_cols_)\n",
    "\n",
    "\n",
    "class LowVarianceDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Usuwa kolumny o niskiej wariancji.\"\"\"\n",
    "\n",
    "    def __init__(self, var_threshold=0.01):\n",
    "        self.var_threshold = var_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        num_cols = X.select_dtypes(include=[np.number, np.bool_]).columns\n",
    "        variances = X[num_cols].var(numeric_only=True)\n",
    "        self.low_var_cols_ = variances[variances < self.var_threshold].index.tolist()\n",
    "        if len(self.low_var_cols_) > 0:\n",
    "            print(f\"‚ö†Ô∏è Zapamiƒôtano {len(self.low_var_cols_)} kolumn o niskiej wariancji (< {self.var_threshold})\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.low_var_cols_, errors='ignore')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po usuniƒôciu kolumn o niskiej wariancji.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array([col for col in input_features if col not in self.low_var_cols_])\n",
    "\n",
    "\n",
    "class HighCorrelationDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Usuwa kolumny silnie skorelowane.\"\"\"\n",
    "\n",
    "    def __init__(self, corr_threshold=0.9):\n",
    "        self.corr_threshold = corr_threshold\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        num_cols = X.select_dtypes(include=[np.number, np.bool_]).columns\n",
    "        corr_matrix = X[num_cols].corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        self.high_corr_cols_ = [col for col in upper.columns if any(upper[col] > self.corr_threshold)]\n",
    "        if len(self.high_corr_cols_) > 0:\n",
    "            print(f\"üîÅ Zapamiƒôtano {len(self.high_corr_cols_)} kolumn z wysokƒÖ korelacjƒÖ (> {self.corr_threshold})\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.high_corr_cols_, errors='ignore')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po usuniƒôciu skorelowanych kolumn.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array([col for col in input_features if col not in self.high_corr_cols_])\n",
    "\n",
    "\n",
    "class WoETransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer zamieniajƒÖcy zmienne numeryczne na WoE wzglƒôdem y (default flag).\n",
    "\n",
    "    Za≈Ço≈ºenia:\n",
    "    - y = 1 -> 'bad' (default)\n",
    "    - y = 0 -> 'good' (brak defaultu)\n",
    "\n",
    "    Dzia≈Ça w dw√≥ch krokach:\n",
    "    1) dzieli ka≈ºdƒÖ kolumnƒô na n_bins kwantylowych przedzia≈Ç√≥w (+ osobny bin na missing),\n",
    "    2) liczy WoE dla ka≈ºdego binu i zapisuje s≈Çowniki mapowa≈Ñ.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=5, eps=0.5):\n",
    "        \"\"\"\n",
    "        n_bins: liczba bin√≥w kwantylowych (bez binu na brak)\n",
    "        eps: smoothing dodawany do licznik√≥w good/bad, ≈ºeby uniknƒÖƒá WoE = +/- inf\n",
    "        \"\"\"\n",
    "        self.n_bins = n_bins\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        y = pd.Series(y)\n",
    "\n",
    "        # bierzemy tylko kolumny numeryczne (WoE ma sens g≈Ç√≥wnie tam)\n",
    "        self.num_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # globalne liczebno≈õci\n",
    "        self.total_good_ = (y == 0).sum()\n",
    "        self.total_bad_ = (y == 1).sum()\n",
    "\n",
    "        self.bin_edges_ = {}\n",
    "        self.woe_maps_ = {}\n",
    "        self.iv_ = {}\n",
    "\n",
    "        for col in self.num_cols_:\n",
    "            col_data = X[col]\n",
    "            df_tmp = pd.DataFrame({\"x\": col_data, \"y\": y})\n",
    "\n",
    "            # osobny bin na braki\n",
    "            missing_mask = df_tmp[\"x\"].isna()\n",
    "\n",
    "            # kwantylowy binning na nie-missing\n",
    "            if (~missing_mask).sum() == 0:\n",
    "                # kolumna w ca≈Ço≈õci pusta -> WoE = 0\n",
    "                self.bin_edges_[col] = None\n",
    "                self.woe_maps_[col] = {\"MISSING\": 0.0}\n",
    "                self.iv_[col] = 0.0\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # retbins=True -> dostajemy krawƒôdzie przedzia≈Ç√≥w\n",
    "                _, bins = pd.qcut(\n",
    "                    df_tmp.loc[~missing_mask, \"x\"],\n",
    "                    q=self.n_bins,\n",
    "                    duplicates=\"drop\",\n",
    "                    retbins=True\n",
    "                )\n",
    "            except ValueError:\n",
    "                # za ma≈Ço unikalnych warto≈õci -> jeden bin\n",
    "                bins = np.unique(df_tmp.loc[~missing_mask, \"x\"])\n",
    "                if bins.size == 1:\n",
    "                    bins = np.array([bins[0] - 1e-6, bins[0] + 1e-6])\n",
    "\n",
    "            self.bin_edges_[col] = bins\n",
    "\n",
    "            # przypisanie bin√≥w\n",
    "            df_tmp[\"bin\"] = pd.cut(\n",
    "                df_tmp[\"x\"],\n",
    "                bins=bins,\n",
    "                include_lowest=True\n",
    "            )\n",
    "            df_tmp[\"bin\"] = df_tmp[\"bin\"].astype(object)\n",
    "\n",
    "            df_tmp.loc[missing_mask, \"bin\"] = \"MISSING\"\n",
    "\n",
    "            # agregacja good/bad per bin\n",
    "            grouped = df_tmp.groupby(\"bin\")[\"y\"]\n",
    "            good = (grouped.apply(lambda s: (s == 0).sum()) + self.eps)\n",
    "            bad = (grouped.apply(lambda s: (s == 1).sum()) + self.eps)\n",
    "\n",
    "            dist_good = good / (self.total_good_ + self.eps * len(good))\n",
    "            dist_bad = bad / (self.total_bad_ + self.eps * len(bad))\n",
    "\n",
    "            woe = np.log(dist_good / dist_bad)\n",
    "\n",
    "            # zapisujemy mapowanie: bin -> WoE\n",
    "            woe_map = woe.to_dict()\n",
    "            self.woe_maps_[col] = woe_map\n",
    "\n",
    "            # policz IV tej zmiennej (przyda siƒô p√≥≈∫niej do raportu)\n",
    "            iv_col = ((dist_good - dist_bad) * woe).sum()\n",
    "            self.iv_[col] = iv_col\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        for col in self.num_cols_:\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "\n",
    "            col_data = X[col]\n",
    "            bins = self.bin_edges_[col]\n",
    "            woe_map = self.woe_maps_[col]\n",
    "\n",
    "            if bins is not None:\n",
    "                binned = pd.cut(\n",
    "                    col_data,\n",
    "                    bins=bins,\n",
    "                    include_lowest=True\n",
    "                ).astype(object)\n",
    "            else:\n",
    "                # kolumna by≈Ça w ca≈Ço≈õci missing przy fit\n",
    "                binned = pd.Series([\"MISSING\"] * len(X), index=X.index, dtype=object)\n",
    "\n",
    "            # missing -> \"MISSING\"\n",
    "            binned[col_data.isna()] = \"MISSING\"\n",
    "\n",
    "            # zamiana bin√≥w na WoE; nieznane biny -> 0.0\n",
    "            X[col] = binned.map(woe_map).fillna(0.0).astype(float)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech - WoE nie zmienia nazw kolumn.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        return np.array(input_features)\n",
    "\n",
    "\n",
    "class WoEDirectionalityFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Dla cech po WoE:\n",
    "    - liczy korelacjƒô (domy≈õlnie Spearmana) z targetem\n",
    "    - zostawia tylko te kolumny, dla kt√≥rych korelacja jest wyra≈∫nie ujemna.\n",
    "      (czyli: wiƒôksze WoE => mniej default√≥w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_corr=-0.01, method=\"spearman\", verbose=True):\n",
    "        \"\"\"\n",
    "        min_corr : float\n",
    "            pr√≥g ujemnej korelacji ‚Äì zostawiamy tylko kolumny z corr < min_corr\n",
    "            np. -0.01 znaczy: zachowaj, je≈õli korelacja <= -0.01\n",
    "        method : {\"spearman\", \"pearson\"}\n",
    "        verbose : bool\n",
    "        \"\"\"\n",
    "        self.min_corr = min_corr\n",
    "        self.method = method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # zadbajmy o DataFrame z nazwami kolumn\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_df = X.copy()\n",
    "        else:\n",
    "            X_df = pd.DataFrame(X, columns=[f\"x_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        self.corrs_ = {}\n",
    "        for col in X_df.columns:\n",
    "            try:\n",
    "                c = X_df[col].corr(y_series, method=self.method)\n",
    "            except Exception:\n",
    "                c = np.nan\n",
    "            self.corrs_[col] = c\n",
    "\n",
    "        # zostawiamy kolumny z wyra≈∫nie ujemnƒÖ korelacjƒÖ\n",
    "        self.cols_to_keep_ = [\n",
    "            col for col, c in self.corrs_.items()\n",
    "            if pd.notna(c) and c < self.min_corr\n",
    "        ]\n",
    "\n",
    "        if self.verbose:\n",
    "            total = X_df.shape[1]\n",
    "            kept = len(self.cols_to_keep_)\n",
    "            dropped = total - kept\n",
    "            print(\n",
    "                f\"üßπ WoEDirectionalityFilter: zachowano {kept}/{total} kolumn, \"\n",
    "                f\"usuniƒôto {dropped} (corr >= {self.min_corr:.3f})\"\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_df = X\n",
    "        else:\n",
    "            # je≈õli X jest macierzƒÖ ‚Äì zak≈Çadamy tƒô samƒÖ kolejno≈õƒá kolumn co w fit\n",
    "            X_df = pd.DataFrame(X, columns=list(self.corrs_.keys()))\n",
    "\n",
    "        return X_df[self.cols_to_keep_]\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po filtrowaniu.\"\"\"\n",
    "        return np.array(self.cols_to_keep_)\n",
    "\n",
    "\n",
    "class DropColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer usuwajƒÖcy wskazane kolumny.\n",
    "\n",
    "    Mo≈ºna:\n",
    "    - przekazaƒá listƒô kolumn w parametrze `columns`\n",
    "    - albo ≈õcie≈ºkƒô do pliku CSV z listƒÖ cech (`columns_path`),\n",
    "      gdzie kolumna z nazwami cech nazywa siƒô np. 'feature'.\n",
    "\n",
    "    U≈ºywamy go przed WoE, ≈ºeby wyrzuciƒá cechy z dodatnimi beta / wysokim VIF.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns=None, columns_path=None, feature_col=\"feature\"):\n",
    "        self.columns = columns\n",
    "        self.columns_path = columns_path\n",
    "        self.feature_col = feature_col\n",
    "        self.columns_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Je≈õli kolumny podane \"na sztywno\"\n",
    "        if self.columns is not None:\n",
    "            self.columns_ = list(self.columns)\n",
    "            return self\n",
    "\n",
    "        # Je≈õli mamy ≈õcie≈ºkƒô do CSV z listƒÖ cech\n",
    "        if self.columns_path is not None:\n",
    "            try:\n",
    "                df_cols = pd.read_csv(self.columns_path)\n",
    "                if self.feature_col not in df_cols.columns:\n",
    "                    raise ValueError(\n",
    "                        f\"Plik {self.columns_path} nie zawiera kolumny '{self.feature_col}' \"\n",
    "                        \"z nazwami cech.\"\n",
    "                    )\n",
    "                self.columns_ = df_cols[self.feature_col].astype(str).tolist()\n",
    "                if len(self.columns_) > 0:\n",
    "                    print(\n",
    "                        f\"üßπ DropColumnsTransformer: zapamiƒôtano {len(self.columns_)} kolumn \"\n",
    "                        f\"do usuniƒôcia z pliku {self.columns_path}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"üßπ DropColumnsTransformer: plik {self.columns_path} jest pusty ‚Äì \"\n",
    "                        \"nie usuwamy ≈ºadnych kolumn.\"\n",
    "                    )\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è DropColumnsTransformer: nie znaleziono pliku {self.columns_path}. \"\n",
    "                    \"Nie usuwamy ≈ºadnych kolumn.\"\n",
    "                )\n",
    "                self.columns_ = []\n",
    "        else:\n",
    "            # Nic nie podano ‚Äì transformer jest no-op\n",
    "            self.columns_ = []\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if not self.columns_:\n",
    "            return X\n",
    "        return X.drop(columns=self.columns_, errors=\"ignore\")\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Zwraca nazwy cech po usuniƒôciu kolumn.\"\"\"\n",
    "        if input_features is None:\n",
    "            return None\n",
    "        if not self.columns_:\n",
    "            return np.array(input_features)\n",
    "        return np.array([col for col in input_features if col not in self.columns_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa03c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopasowanie_pipeline.py\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "# ...reszta import√≥w (sklearn, transformers itd.)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ KONFIGURACJA ≈öCIE≈ªEK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))        # .../IWUM-Projekt-1/EDA\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\")) # .../IWUM-Projekt-1\n",
    "\n",
    "# ≈ºeby import transformers.py z tego folderu zawsze dzia≈Ça≈Ç\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.append(BASE_DIR)\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(BASE_DIR, \"preprocesing_pipelines\")  # dok≈Çadnie tak, jak folder siƒô nazywa u Ciebie\n",
    "os.makedirs(PREPROC_DIR, exist_ok=True)\n",
    "\n",
    "INTERP_LOGIT_DIR = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"Modele_interpretowalne\",\n",
    "    \"interpretowalnosc_logit\",\n",
    ")\n",
    "FEATURES_TO_DROP_PATH = os.path.join(INTERP_LOGIT_DIR, \"logit_features_to_drop.csv\")\n",
    "FEATURES_TO_DROP_PATH_K5 = os.path.join(INTERP_LOGIT_DIR, \"drop_columns_k5.csv\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from eda_transformers import (\n",
    "    InfinityReplacer,\n",
    "    HighMissingDropper,\n",
    "    MissingIndicator,\n",
    "    CustomImputer,\n",
    "    Winsorizer,\n",
    "    LowVarianceDropper,\n",
    "    HighCorrelationDropper,\n",
    "    OneHotEncoder,\n",
    "    NumericScaler,   # mo≈ºe siƒô jeszcze przydaƒá, na razie nie u≈ºywamy\n",
    "    WoETransformer,   # NOWY transformer, musi byƒá dodany w transformers.py\n",
    "    WoEDirectionalityFilter,  # potrzebny zeby logit byl interpretowalny\n",
    "    DropColumnsTransformer\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ========= PIPELINE DLA DRZEWA DECYZYJNEGO =========\n",
    "\n",
    "def create_tree_preprocessing_pipeline(\n",
    "    missing_threshold: float = 0.95,\n",
    "    lower_q: float = 0.02,\n",
    "    upper_q: float = 0.98,\n",
    "    var_threshold: float = 0.01,\n",
    "    corr_threshold: float = 0.9,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Preprocessing pod drzewo:\n",
    "    - OneHotEncoder dla zmiennych kategorycznych\n",
    "    - zamiana inf na NaN\n",
    "    - wyrzucenie kolumn z ogromnƒÖ liczbƒÖ brak√≥w\n",
    "    - dodanie wska≈∫nik√≥w brak√≥w\n",
    "    - imputacja (median / most_frequent)\n",
    "    - winsoryzacja (obciƒôcie outlier√≥w)\n",
    "    - wyrzucenie kolumn o bardzo ma≈Çej wariancji\n",
    "    - wyrzucenie kolumn mocno skorelowanych\n",
    "    - BEZ skalowania (drzewo go nie potrzebuje)\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        (\"one_hot\", OneHotEncoder()),\n",
    "        (\"inf_replacer\", InfinityReplacer()),\n",
    "        (\"drop_high_missing\", HighMissingDropper(missing_threshold=missing_threshold)),\n",
    "        (\"missing_indicator\", MissingIndicator()),\n",
    "        (\"imputer\", CustomImputer()),\n",
    "        (\"winsorizer\", Winsorizer(lower_q=lower_q, upper_q=upper_q)),\n",
    "        (\"drop_low_variance\", LowVarianceDropper(var_threshold=var_threshold)),\n",
    "        (\"drop_high_corr\", HighCorrelationDropper(corr_threshold=corr_threshold)),\n",
    "    ]\n",
    "\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "# ========= PIPELINE DLA REGRESJI LOGISTYCZNEJ (WoE) =========\n",
    "\n",
    "def create_logit_preprocessing_pipeline(\n",
    "    missing_threshold: float = 0.95,\n",
    "    lower_q: float = 0.02,\n",
    "    upper_q: float = 0.98,\n",
    "    var_threshold: float = 0.01,\n",
    "    corr_threshold: float = 0.9,\n",
    "    n_bins: int = 5,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Preprocessing pod regresjƒô logistycznƒÖ z WoE:\n",
    "    - OneHotEncoder (na razie zostawiamy, bo drzewo te≈º go ma; mo≈ºna p√≥≈∫niej upro≈õciƒá)\n",
    "    - zamiana inf na NaN\n",
    "    - wyrzucenie kolumn z ogromnƒÖ liczbƒÖ brak√≥w\n",
    "    - dodanie wska≈∫nik√≥w brak√≥w\n",
    "    - imputacja (median / most_frequent)\n",
    "    - winsoryzacja\n",
    "    - wyrzucenie kolumn o bardzo ma≈Çej wariancji\n",
    "    - WoETransformer (binning + WoE na zmiennych numerycznych)\n",
    "    - wyrzucenie kolumn mocno skorelowanych JU≈ª po WoE\n",
    "    - BEZ skalowania (WoE jest ju≈º na sensownej skali)\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        (\"one_hot\", OneHotEncoder()),\n",
    "        (\"inf_replacer\", InfinityReplacer()),\n",
    "        (\"drop_high_missing\", HighMissingDropper(missing_threshold=missing_threshold)),\n",
    "        (\"missing_indicator\", MissingIndicator()),\n",
    "        (\"imputer\", CustomImputer()),\n",
    "        (\"winsorizer\", Winsorizer(lower_q=lower_q, upper_q=upper_q)),\n",
    "        (\"drop_low_variance\", LowVarianceDropper(var_threshold=var_threshold)),\n",
    "        (\"drop_high_corr\", HighCorrelationDropper(corr_threshold=corr_threshold)),\n",
    "        (\"woe\", WoETransformer(n_bins=n_bins)),\n",
    "        (\"woe_directionality\", WoEDirectionalityFilter(min_corr=-0.01, method=\"spearman\")),\n",
    "        (\"drop_bad_for_logit\", DropColumnsTransformer(columns_path=FEATURES_TO_DROP_PATH)),\n",
    "        (\"drop_unnessesary_for_logit\", DropColumnsTransformer(columns_path=FEATURES_TO_DROP_PATH_K5)),\n",
    "    ]\n",
    "\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "# ========= PIPELINE DLA MODELI NIEINTERPRETOWALNYCH (XGBoost, LightGBM, MLP) =========\n",
    "\n",
    "def create_blackbox_preprocessing_pipeline(\n",
    "    missing_threshold: float = 0.95,\n",
    "    lower_q: float = 0.02,\n",
    "    upper_q: float = 0.98,\n",
    "    var_threshold: float = 0.01,\n",
    "    corr_threshold: float = 0.9,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Preprocessing pod modele nieinterpretowalne (XGBoost, LightGBM, MLP):\n",
    "    - OneHotEncoder dla zmiennych kategorycznych\n",
    "    - zamiana inf na NaN\n",
    "    - wyrzucenie kolumn z ogromnƒÖ liczbƒÖ brak√≥w\n",
    "    - dodanie wska≈∫nik√≥w brak√≥w\n",
    "    - imputacja (median / most_frequent)\n",
    "    - winsoryzacja (obciƒôcie outlier√≥w)\n",
    "    - wyrzucenie kolumn o bardzo ma≈Çej wariancji\n",
    "    - wyrzucenie kolumn mocno skorelowanych\n",
    "    - NumericScaler (standaryzacja dla MLP - dla drzew nie szkodzi)\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        (\"one_hot\", OneHotEncoder()),\n",
    "        (\"inf_replacer\", InfinityReplacer()),\n",
    "        (\"drop_high_missing\", HighMissingDropper(missing_threshold=missing_threshold)),\n",
    "        (\"missing_indicator\", MissingIndicator()),\n",
    "        (\"imputer\", CustomImputer()),\n",
    "        (\"winsorizer\", Winsorizer(lower_q=lower_q, upper_q=upper_q)),\n",
    "        (\"drop_low_variance\", LowVarianceDropper(var_threshold=var_threshold)),\n",
    "        (\"drop_high_corr\", HighCorrelationDropper(corr_threshold=corr_threshold)),\n",
    "        (\"scaler\", NumericScaler()),  # Dodajemy skalowanie dla MLP\n",
    "    ]\n",
    "\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# ========= G≈Å√ìWNY BLOK: PODZIA≈Å DANYCH + FITOWANIE PIPELINE‚Äô√ìW =========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Wczytanie danych\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    # Zak≈Çadamy, ≈ºe kolumna celu to 'default'\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    print(\"Rozmiar pe≈Çnego zbioru:\", X.shape)\n",
    "\n",
    "    # 2. Podzia≈Ç train / temp / test (60 / 20 / 20) ze sta≈Çym random_state\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.4,\n",
    "        stratify=y,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=0.5,\n",
    "        stratify=y_temp,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    # 3. Tworzymy oba pipeline‚Äôy\n",
    "    tree_pipeline = create_tree_preprocessing_pipeline(\n",
    "        missing_threshold=0.95,\n",
    "        lower_q=0.02,\n",
    "        upper_q=0.98,\n",
    "        var_threshold=0.01,\n",
    "        corr_threshold=0.9,\n",
    "    )\n",
    "\n",
    "    logit_pipeline = create_logit_preprocessing_pipeline(\n",
    "        missing_threshold=0.95,\n",
    "        lower_q=0.02,\n",
    "        upper_q=0.98,\n",
    "        var_threshold=0.01,\n",
    "        corr_threshold=0.9,\n",
    "        n_bins=5,\n",
    "    )\n",
    "        # Pipeline dla modeli nieinterpretowalnych\n",
    "    blackbox_pipeline = create_blackbox_preprocessing_pipeline(\n",
    "        missing_threshold=0.95,\n",
    "        lower_q=0.02,\n",
    "        upper_q=0.98,\n",
    "        var_threshold=0.01,\n",
    "        corr_threshold=0.9,\n",
    "    )\n",
    "\n",
    "    # 4. Fitujemy pipeline‚Äôy na zbiorze treningowym\n",
    "    print(\"\\n Fitowanie pipeline‚Äôu dla drzewa na zbiorze treningowym...\")\n",
    "    X_train_tree = tree_pipeline.fit_transform(X_train, y_train)\n",
    "    print(\"Kszta≈Çt po przetworzeniu (drzewo):\", X_train_tree.shape)\n",
    "\n",
    "    print(\"\\n Fitowanie pipeline‚Äôu dla logitu (WoE) na zbiorze treningowym...\")\n",
    "    X_train_logit = logit_pipeline.fit_transform(X_train, y_train)\n",
    "    print(\"Kszta≈Çt po przetworzeniu (logit+WoE):\", X_train_logit.shape)\n",
    "    \n",
    "    print(\"\\n Fitowanie pipeline'u dla modeli nieinterpretowalnych na zbiorze treningowym...\")\n",
    "    X_train_blackbox = blackbox_pipeline.fit_transform(X_train, y_train)\n",
    "    print(\"Kszta≈Çt po przetworzeniu (blackbox):\", X_train_blackbox.shape)\n",
    "\n",
    "\n",
    "    # 5. Zapisujemy pipeline‚Äôy do plik√≥w\n",
    "    joblib.dump(tree_pipeline, os.path.join(PREPROC_DIR, \"preprocessing_tree.pkl\"))\n",
    "    joblib.dump(logit_pipeline, os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\"))\n",
    "    joblib.dump(blackbox_pipeline, os.path.join(PREPROC_DIR, \"preprocessing_blackbox.pkl\"))\n",
    "\n",
    "    print(\"\\n Zapisano pipeline'y:\")\n",
    "    print(\"   - preprocessing_tree.pkl\")\n",
    "    print(\"   - preprocessing_logit_woe.pkl\")\n",
    "    print(\"   - preprocessing_blackbox.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele_interpretowalne.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "# reszta import√≥w sklearn, warnings, itd.\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ KONFIGURACJA ≈öCIE≈ªEK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))        # .../IWUM-Projekt-1/Modele_interpretowalne\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\")) # .../IWUM-Projekt-1\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(PROJECT_ROOT, \"EDA\", \"preprocesing_pipelines\")\n",
    "\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"model_results\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from scipy.stats import ks_2samp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =====================================================================\n",
    "#                            CUSTOM METRICS\n",
    "# =====================================================================\n",
    "\n",
    "def gini_from_auc(auc):\n",
    "    return 2 * auc - 1\n",
    "\n",
    "def calculate_ks_statistic(y_true, y_pred_proba):\n",
    "    \"\"\"Kolmogorov-Smirnov statistic.\"\"\"\n",
    "    data = pd.DataFrame({\"y\": y_true, \"p\": y_pred_proba}).sort_values(\"p\")\n",
    "\n",
    "    pos_probs = data.loc[data[\"y\"] == 1, \"p\"]\n",
    "    neg_probs = data.loc[data[\"y\"] == 0, \"p\"]\n",
    "\n",
    "    if len(pos_probs) == 0 or len(neg_probs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    ks_stat, _ = ks_2samp(pos_probs, neg_probs)\n",
    "    return ks_stat\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                        GRIDY HIPERPARAMETR√ìW\n",
    "# =====================================================================\n",
    "\n",
    "def create_logistic_regression_grid():\n",
    "    \"\"\"\n",
    "    Logit na WoE ‚Äî legalne kombinacje penalty/solver:\n",
    "    - L2 + lbfgs / newton-cg\n",
    "    - L1 + saga / liblinear\n",
    "    - Elasticnet + saga\n",
    "    \"\"\"\n",
    "    base_model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\", \"newton-cg\"],\n",
    "            \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        {\n",
    "            \"penalty\": [\"l1\"],\n",
    "            \"solver\": [\"liblinear\", \"saga\"],\n",
    "            \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        {\n",
    "            \"penalty\": [\"elasticnet\"],\n",
    "            \"solver\": [\"saga\"],\n",
    "            \"l1_ratio\": [0.3, 0.5, 0.7],\n",
    "            \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return base_model, param_grid\n",
    "\n",
    "\n",
    "def create_decision_tree_grid():\n",
    "    \"\"\"\n",
    "    Drzewo interpretowalne (p≈Çytkie) + pruning.\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        \"max_depth\": [3, 4, 5, 7, 10],\n",
    "        \"min_samples_split\": [20, 50, 100],\n",
    "        \"min_samples_leaf\": [20, 50, 100],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"class_weight\": [None, \"balanced\"],\n",
    "        \"ccp_alpha\": [0.0, 0.001, 0.01],\n",
    "    }\n",
    "\n",
    "    return model, param_grid\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                           EWALUACJA MODELI\n",
    "# =====================================================================\n",
    "\n",
    "def evaluate_model(model, X, y, model_name=\"Model\", dataset_name=\"val\"):\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    roc = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"roc_auc\": roc,\n",
    "        \"gini\": 2 * roc - 1,\n",
    "        \"pr_auc\": average_precision_score(y, y_pred_proba),\n",
    "        \"ks\": calculate_ks_statistic(y, y_pred_proba),\n",
    "        \"log_loss\": log_loss(y, y_pred_proba),\n",
    "        \"brier\": brier_score_loss(y, y_pred_proba),\n",
    "    }\n",
    "\n",
    "def print_evaluation_table(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"              WYNIKI MODELI (VAL + TEST)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                         GRIDSEARCH DLA MODELU\n",
    "# =====================================================================\n",
    "\n",
    "def train_with_gridsearch(\n",
    "    model, param_grid, X_train, y_train, model_name=\"Model\", cv=5\n",
    "):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\" GridSearch: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",  #  tylko ROC-AUC, ≈ºadnych custom scorer√≥w\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nNajlepsze parametry:\")\n",
    "    print(gs.best_params_)\n",
    "    print(f\"Najlepszy ROC-AUC CV: {gs.best_score_:.4f}\")\n",
    "\n",
    "    return gs.best_estimator_, gs\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                                MAIN\n",
    "# =====================================================================\n",
    "\n",
    "def main():\n",
    "    print(\" Wczytywanie danych...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    # Podzia≈Ç jak w EDA.py ‚Äî 60/20/20\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    print(\"\\n ≈Åadowanie pipeline‚Äô√≥w...\")\n",
    "    tree_preproc = joblib.load(os.path.join(PREPROC_DIR, \"preprocessing_tree.pkl\"))\n",
    "    logit_preproc = joblib.load(os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\"))\n",
    "\n",
    "    print(\"\\n Transformacja danych dla drzewa...\")\n",
    "    X_train_tree = tree_preproc.transform(X_train)\n",
    "    X_val_tree = tree_preproc.transform(X_val)\n",
    "    X_test_tree = tree_preproc.transform(X_test)\n",
    "\n",
    "    print(\"\\n Transformacja danych dla logitu...\")\n",
    "    X_train_logit = logit_preproc.transform(X_train)\n",
    "    X_val_logit = logit_preproc.transform(X_val)\n",
    "    X_test_logit = logit_preproc.transform(X_test)\n",
    "\n",
    "    # ============================\n",
    "    #       GRIDSEARCH LOGIT\n",
    "    # ============================\n",
    "    logit_model, logit_grid = create_logistic_regression_grid()\n",
    "    best_logit, gs_logit = train_with_gridsearch(\n",
    "        logit_model, logit_grid, X_train_logit, y_train, \"Logit (WoE)\", cv=5\n",
    "    )\n",
    "\n",
    "    # ============================\n",
    "    #       GRIDSEARCH DRZEWO\n",
    "    # ============================\n",
    "    tree_model, tree_grid = create_decision_tree_grid()\n",
    "    best_tree, gs_tree = train_with_gridsearch(\n",
    "        tree_model, tree_grid, X_train_tree, y_train, \"Decision Tree\", cv=5\n",
    "    )\n",
    "\n",
    "    # ============================\n",
    "    #            EWALUACJA\n",
    "    # ============================\n",
    "    results = []\n",
    "\n",
    "    # logit\n",
    "    results.append(evaluate_model(best_logit, X_val_logit, y_val, \"Logit_WoE\", \"val\"))\n",
    "    results.append(evaluate_model(best_logit, X_test_logit, y_test, \"Logit_WoE\", \"test\"))\n",
    "\n",
    "    # drzewo\n",
    "    results.append(evaluate_model(best_tree, X_val_tree, y_val, \"DecisionTree\", \"val\"))\n",
    "    results.append(evaluate_model(best_tree, X_test_tree, y_test, \"DecisionTree\", \"test\"))\n",
    "\n",
    "    df_results = print_evaluation_table(results)\n",
    "\n",
    "    # ============================\n",
    "    #             ZAPIS\n",
    "    # ============================\n",
    "    print(\"\\n Zapisujemy modele...\")\n",
    "\n",
    "    joblib.dump(best_logit, os.path.join(MODELS_DIR, \"best_logistic_regression_woe.pkl\"))\n",
    "    joblib.dump(best_tree, os.path.join(MODELS_DIR, \"best_decision_tree.pkl\"))\n",
    "    \n",
    "    df_results.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"model_evaluation_results.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    \n",
    "    pd.DataFrame(gs_logit.cv_results_).to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"grid_results_logit_woe.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(gs_tree.cv_results_).to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"grid_results_tree.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n================ BETA COEFFICIENTS ================\\n\")\n",
    "\n",
    "    # pobierz nazwy zmiennych po transformacji WOE + DropColumns\n",
    "    woe_feature_names = logit_preproc.get_feature_names_out()\n",
    "    \n",
    "    # ale DropColumnsTransformer uciƒÖ≈Ç kolumny ‚Äî wiƒôc\n",
    "    # pobieramy REALNE nazwy cech po transformacji\n",
    "    X_logit_df = pd.DataFrame(X_train_logit)\n",
    "    feature_names = list(X_logit_df.columns)\n",
    "    \n",
    "    # wsp√≥≈Çczynniki\n",
    "    betas = best_logit.coef_[0]\n",
    "    intercept = best_logit.intercept_[0]\n",
    "    \n",
    "    print(f\"Intercept (Œ≤0): {intercept:.6f}\\n\")\n",
    "    \n",
    "    for fname, beta in zip(feature_names, betas):\n",
    "        print(f\"{fname:40s}  Œ≤ = {beta:.6f}\")\n",
    "\n",
    "    print(\"Zapisano wszystkie modele i wyniki.\")\n",
    "\n",
    "    return best_logit, best_tree, df_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocena_jakosci_modelow_wykresy.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ KONFIGURACJA ≈öCIE≈ªEK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))        # .../IWUM-Projekt-1/Modele_interpretowalne\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\")) # .../IWUM-Projekt-1\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(PROJECT_ROOT, \"EDA\", \"preprocesing_pipelines\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"wykresy_oceny_jakosci\")\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                 FUNKCJE POMOCNICZE / METRYKI DODATKOWE\n",
    "# =====================================================================\n",
    "\n",
    "def calculate_ks_statistic(y_true, y_pred_proba):\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_pred_proba}).sort_values(\"p\")\n",
    "    pos = df.loc[df[\"y\"] == 1, \"p\"]\n",
    "    neg = df.loc[df[\"y\"] == 0, \"p\"]\n",
    "\n",
    "    if len(pos) == 0 or len(neg) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    from scipy.stats import ks_2samp\n",
    "    ks_stat, _ = ks_2samp(pos, neg)\n",
    "    return ks_stat\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                            WYKRESY\n",
    "# =====================================================================\n",
    "\n",
    "def plot_roc(models, savepath=None):\n",
    "    \"\"\"ROC curves dla obu modeli (val + test).\"\"\"\n",
    "    if savepath is None:\n",
    "        savepath = os.path.join(PLOTS_DIR, \"roc_logit_tree.png\")\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for name, y_val, p_val, y_test, p_test in models:\n",
    "        fpr_val, tpr_val, _ = roc_curve(y_val, p_val)\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, p_test)\n",
    "\n",
    "        auc_val = roc_auc_score(y_val, p_val)\n",
    "        auc_test = roc_auc_score(y_test, p_test)\n",
    "\n",
    "        plt.plot(fpr_val, tpr_val, label=f\"{name} ‚Äì Val (AUC={auc_val:.3f})\")\n",
    "        plt.plot(fpr_test, tpr_test, label=f\"{name} ‚Äì Test (AUC={auc_test:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"--\", label=\"Losowy model\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"ROC curve ‚Äì Logit (WoE) vs Decision Tree\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pr(models, savepath=None):\n",
    "    \"\"\"Precision‚ÄìRecall curves dla obu modeli (val + test).\"\"\"\n",
    "    if savepath is None:\n",
    "        savepath = os.path.join(PLOTS_DIR, \"pr_logit_tree.png\")\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "\n",
    "    for name, y_val, p_val, y_test, p_test in models:\n",
    "        prec_val, rec_val, _ = precision_recall_curve(y_val, p_val)\n",
    "        prec_test, rec_test, _ = precision_recall_curve(y_test, p_test)\n",
    "\n",
    "        ap_val = average_precision_score(y_val, p_val)\n",
    "        ap_test = average_precision_score(y_test, p_test)\n",
    "\n",
    "        plt.plot(rec_val, prec_val, label=f\"{name} ‚Äì Val (AP={ap_val:.3f})\")\n",
    "        plt.plot(rec_test, prec_test, label=f\"{name} ‚Äì Test (AP={ap_test:.3f})\")\n",
    "\n",
    "    baseline = models[0][1].mean()\n",
    "    plt.hlines(baseline, 0, 1, linestyles=\"--\", label=f\"Baseline ({baseline:.3f})\")\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"PR curve ‚Äì Logit (WoE) vs Decision Tree\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_calibration(models, savepath=None, n_bins=10):\n",
    "    \"\"\"Calibration / reliability plot dla obu modeli (val + test).\"\"\"\n",
    "    if savepath is None:\n",
    "        savepath = os.path.join(PLOTS_DIR, \"calibration_logit_tree.png\")\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot([0, 1], [0, 1], \"--\", label=\"Idealna kalibracja\")\n",
    "\n",
    "    for name, y_val, p_val, y_test, p_test in models:\n",
    "        pt_val, pp_val = calibration_curve(y_val, p_val, n_bins=n_bins)\n",
    "        pt_test, pp_test = calibration_curve(y_test, p_test, n_bins=n_bins)\n",
    "\n",
    "        plt.plot(pp_val, pt_val, \"o-\", label=f\"{name} ‚Äì Val\")\n",
    "        plt.plot(pp_test, pt_test, \"s-\", label=f\"{name} ‚Äì Test\")\n",
    "\n",
    "    plt.xlabel(\"≈örednie przewidziane PD (bin)\")\n",
    "    plt.ylabel(\"Rzeczywista czƒôsto≈õƒá default√≥w\")\n",
    "    plt.title(\"Calibration ‚Äì Logit (WoE) vs Decision Tree\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_hist(models, out_dir=None):\n",
    "    \"\"\"Histogramy PD dla good/bad osobno dla ka≈ºdego modelu i zbioru.\"\"\"\n",
    "    if out_dir is None:\n",
    "        out_dir = PLOTS_DIR\n",
    "\n",
    "    for name, y_val, p_val, y_test, p_test in models:\n",
    "        for ds_name, y, p in [(\"val\", y_val, p_val), (\"test\", y_test, p_test)]:\n",
    "            plt.figure(figsize=(7, 6))\n",
    "            df = pd.DataFrame({\"y\": y, \"p\": p})\n",
    "\n",
    "            plt.hist(\n",
    "                df[df[\"y\"] == 0][\"p\"],\n",
    "                bins=20,\n",
    "                alpha=0.6,\n",
    "                density=True,\n",
    "                label=\"Good\",\n",
    "            )\n",
    "            plt.hist(\n",
    "                df[df[\"y\"] == 1][\"p\"],\n",
    "                bins=20,\n",
    "                alpha=0.6,\n",
    "                density=True,\n",
    "                label=\"Bad\",\n",
    "            )\n",
    "\n",
    "            plt.xlabel(\"Przewidywane PD\")\n",
    "            plt.ylabel(\"Gƒôsto≈õƒá\")\n",
    "            plt.title(f\"Histogram PD ‚Äì {name} ‚Äì {ds_name}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            fname = f\"hist_{name}_{ds_name}.png\"\n",
    "            plt.savefig(os.path.join(out_dir, fname), dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                                MAIN\n",
    "# =====================================================================\n",
    "\n",
    "def main():\n",
    "    print(\" Wczytywanie danych i modeli...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    # modele\n",
    "    logit = joblib.load(os.path.join(MODELS_DIR, \"best_logistic_regression_woe.pkl\"))\n",
    "    tree = joblib.load(os.path.join(MODELS_DIR, \"best_decision_tree.pkl\"))\n",
    "\n",
    "    # preprocessing\n",
    "    preproc_logit = joblib.load(os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\"))\n",
    "    preproc_tree = joblib.load(os.path.join(PREPROC_DIR, \"preprocessing_tree.pkl\"))\n",
    "\n",
    "    # podzia≈Ç danych (jak w innych plikach)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # transformacje\n",
    "    X_val_logit = preproc_logit.transform(X_val)\n",
    "    X_test_logit = preproc_logit.transform(X_test)\n",
    "\n",
    "    X_val_tree = preproc_tree.transform(X_val)\n",
    "    X_test_tree = preproc_tree.transform(X_test)\n",
    "\n",
    "    # predykcje\n",
    "    p_val_logit = logit.predict_proba(X_val_logit)[:, 1]\n",
    "    p_test_logit = logit.predict_proba(X_test_logit)[:, 1]\n",
    "\n",
    "    p_val_tree = tree.predict_proba(X_val_tree)[:, 1]\n",
    "    p_test_tree = tree.predict_proba(X_test_tree)[:, 1]\n",
    "\n",
    "    # pakujemy modele do listy dla wygody\n",
    "    MODELE = [\n",
    "        (\"Logit\", y_val, p_val_logit, y_test, p_test_logit),\n",
    "        (\"Tree\",  y_val, p_val_tree,  y_test, p_test_tree),\n",
    "    ]\n",
    "\n",
    "    # =====================================================================\n",
    "    #                          WYKRESY\n",
    "    # =====================================================================\n",
    "    print(\" Rysujƒô ROC...\")\n",
    "    plot_roc(MODELE)\n",
    "\n",
    "    print(\" Rysujƒô PR...\")\n",
    "    plot_pr(MODELE)\n",
    "\n",
    "    print(\" Rysujƒô calibration...\")\n",
    "    plot_calibration(MODELE)\n",
    "\n",
    "    print(\" Rysujƒô histogramy PD...\")\n",
    "    plot_hist(MODELE)\n",
    "\n",
    "    print(\" Wygenerowano wykresy dla logitu i drzewca!\")\n",
    "    print(f\"   Pliki zapisane w: {PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpretowalnosc_regresja_logistyczna.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "#                KONFIGURACJA ≈öCIE≈ªEK\n",
    "# ============================================================\n",
    "\n",
    "# Ten plik zak≈Çadamy, ≈ºe le≈ºy w:\n",
    "#   .../IWUM-Projekt-1/Modele_interpretowalne/interpretowalnosc_regresja_logistyczna.py\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))        # .../Modele_interpretowalne\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\")) # .../IWUM-Projekt-1\n",
    "\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "INTERP_DIR = os.path.join(BASE_DIR, \"interpretowalnosc_logit\")\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(PROJECT_ROOT, \"EDA\", \"preprocesing_pipelines\")\n",
    "PREPROC_LOGIT_PATH = os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\")\n",
    "\n",
    "# podfoldery na wykresy\n",
    "WYKRESY_DIR = os.path.join(INTERP_DIR, \"waznosc_cech\")\n",
    "PDP_DIR = os.path.join(INTERP_DIR, \"PDP\")\n",
    "ICE_DIR = os.path.join(INTERP_DIR, \"ICE\")\n",
    "WOE_PROFILS = os.path.join(INTERP_DIR, \"woe_profils\")\n",
    "\n",
    "os.makedirs(WOE_PROFILS, exist_ok=True)\n",
    "os.makedirs(INTERP_DIR, exist_ok=True)\n",
    "os.makedirs(WYKRESY_DIR, exist_ok=True)\n",
    "os.makedirs(PDP_DIR, exist_ok=True)\n",
    "os.makedirs(ICE_DIR, exist_ok=True)\n",
    "\n",
    "INTERP_LOCAL_DIR = os.path.join(INTERP_DIR, \"interpretowalnosc_lokalna\")\n",
    "os.makedirs(INTERP_LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "#                ANALIZA WSP√ì≈ÅCZYNNIK√ìW LOGITU\n",
    "# ============================================================\n",
    "\n",
    "def load_logit_model():\n",
    "    model_path = os.path.join(MODELS_DIR, \"best_logistic_regression_woe.pkl\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono modelu logitu pod ≈õcie≈ºkƒÖ: {model_path}\")\n",
    "    logit = joblib.load(model_path)\n",
    "    return logit\n",
    "\n",
    "\n",
    "def load_logit_preprocessor():\n",
    "    if not os.path.exists(PREPROC_LOGIT_PATH):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pipeline'u logitu: {PREPROC_LOGIT_PATH}\")\n",
    "    return joblib.load(PREPROC_LOGIT_PATH)\n",
    "\n",
    "\n",
    "def extract_coefficients(logit):\n",
    "    \"\"\"\n",
    "    Zwraca DataFrame z:\n",
    "        - nazwƒÖ cechy\n",
    "        - beta\n",
    "        - |beta|\n",
    "        - znakiem\n",
    "        - odds_ratio = exp(beta)\n",
    "    \"\"\"\n",
    "    coef = logit.coef_.ravel()\n",
    "    intercept = float(logit.intercept_[0])\n",
    "\n",
    "    if hasattr(logit, \"feature_names_in_\"):\n",
    "        feature_names = np.array(logit.feature_names_in_)\n",
    "    else:\n",
    "        feature_names = np.array([f\"x_{i}\" for i in range(len(coef))])\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"beta\": coef,\n",
    "    })\n",
    "    df_coef[\"abs_beta\"] = df_coef[\"beta\"].abs()\n",
    "    df_coef[\"sign\"] = np.where(\n",
    "        df_coef[\"beta\"] > 0, \"positive\",\n",
    "        np.where(df_coef[\"beta\"] < 0, \"negative\", \"zero\")\n",
    "    )\n",
    "    df_coef[\"odds_ratio\"] = np.exp(df_coef[\"beta\"])\n",
    "\n",
    "    df_coef = df_coef.sort_values(\"abs_beta\", ascending=False).reset_index(drop=True)\n",
    "    return df_coef, intercept\n",
    "\n",
    "\n",
    "def summarize_signs(df_coef, intercept):\n",
    "    n_total = len(df_coef)\n",
    "    n_pos = (df_coef[\"sign\"] == \"positive\").sum()\n",
    "    n_neg = (df_coef[\"sign\"] == \"negative\").sum()\n",
    "    n_zero = (df_coef[\"sign\"] == \"zero\").sum()\n",
    "\n",
    "    print(\"\\n============================\")\n",
    "    print(\"   PODSUMOWANIE WSP√ì≈ÅCZYNNIK√ìW LOGITU\")\n",
    "    print(\"============================\")\n",
    "    print(f\"Intercept (Œ≤0): {intercept:.4f}\")\n",
    "    print(f\"Liczba cech: {n_total}\")\n",
    "    print(f\"  ‚Ä¢ beta > 0  (positive): {n_pos}\")\n",
    "    print(f\"  ‚Ä¢ beta < 0  (negative): {n_neg}\")\n",
    "    print(f\"  ‚Ä¢ beta = 0  (zero):     {n_zero}\")\n",
    "\n",
    "    if n_pos == 0 and n_neg > 0:\n",
    "        print(\"\\n Wszystkie niezerowe bety sƒÖ ujemne ‚Äì kierunek wp≈Çywu jest sp√≥jny z WoE.\")\n",
    "    elif n_neg == 0 and n_pos > 0:\n",
    "        print(\"\\n Wszystkie niezerowe bety sƒÖ dodatnie ‚Äì to oznacza odwrotnƒÖ konwencjƒô WoE.\")\n",
    "    else:\n",
    "        print(\"\\n Mamy mieszane znaki beta ‚Äì warto sprawdziƒá, kt√≥re cechy majƒÖ 'dziwny' kierunek.\")\n",
    "        print(\"   (np. problem z binningiem, korelacjami lub zmiennymi pomocniczymi).\")\n",
    "\n",
    "\n",
    "def save_coefficients(df_coef):\n",
    "    out_path = os.path.join(INTERP_DIR, \"coefficients_logit.csv\")\n",
    "    df_coef.to_csv(out_path, index=False)\n",
    "    print(f\"\\n Zapisano tabelƒô wsp√≥≈Çczynnik√≥w do: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                POBRANIE I PRZETWORZENIE DANYCH\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prepare_data(preproc_logit, logit):\n",
    "    \"\"\"Wczytuje pe≈Çne dane, dzieli na X, y, przepuszcza przez pipeline WoE\n",
    "    i zwraca (X_woe_df, y_series).\"\"\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"].astype(int)\n",
    "\n",
    "    X_woe = preproc_logit.transform(X)\n",
    "\n",
    "    # wymuszamy DataFrame z nazwami cech jak w logit\n",
    "    feature_names = logit.feature_names_in_\n",
    "    X_woe_df = pd.DataFrame(X_woe, columns=feature_names)\n",
    "\n",
    "    return X_woe_df, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                       PROFILE WoE\n",
    "# ============================================================\n",
    "\n",
    "def plot_woe_profile(X_woe, y, feature, save_path):\n",
    "    \"\"\"\n",
    "    Tworzy profil WoE dla danej cechy:\n",
    "       - o≈õ X: warto≈õƒá WoE (po binningu)\n",
    "       - o≈õ Y: czƒôsto≈õƒá default√≥w w danym binie\n",
    "       - linia przerywana: ≈õredni default rate\n",
    "       - pionowa linia w WoE = 0\n",
    "    \"\"\"\n",
    "    df_tmp = pd.DataFrame({\n",
    "        \"woe\": X_woe[feature],\n",
    "        \"y\": y\n",
    "    })\n",
    "\n",
    "    # grupujemy po unikalnych warto≈õciach WoE (to de facto biny)\n",
    "    grp = (\n",
    "        df_tmp\n",
    "        .groupby(\"woe\")\n",
    "        .agg(events=(\"y\", \"sum\"), total=(\"y\", \"count\"))\n",
    "        .reset_index()\n",
    "        .sort_values(\"woe\")\n",
    "    )\n",
    "    grp[\"dr\"] = grp[\"events\"] / grp[\"total\"]\n",
    "\n",
    "    mean_dr = y.mean()\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(grp[\"woe\"], grp[\"dr\"], \"o-\", label=\"default rate\")\n",
    "    plt.axhline(mean_dr, color=\"tab:blue\", linestyle=\"--\",\n",
    "                label=f\"≈öredni default (train) = {mean_dr:.3f}\")\n",
    "    plt.axvline(0.0, color=\"tab:blue\", linestyle=\":\",\n",
    "                label=\"WoE = 0\")\n",
    "\n",
    "    plt.xlabel(\"Warto≈õƒá WoE (po binningu)\")\n",
    "    plt.ylabel(\"Czƒôsto≈õƒá default√≥w w binie\")\n",
    "    plt.title(f\"Profil WoE ‚Äì {feature}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"   ‚ûú zapisano profil WoE: {save_path}\")\n",
    "\n",
    "\n",
    "def generate_woe_profiles(df_coef, X_woe, y):\n",
    "    \"\"\"Rysuje:\n",
    "       ‚Ä¢ profil dla jednej cechy z beta > 0 (je≈õli istnieje),\n",
    "       ‚Ä¢ profile dla 9 cech z najwiƒôkszym |beta| i znakiem negative.\n",
    "    \"\"\"\n",
    "    # cecha z dodatniƒÖ betƒÖ (je≈õli jest)\n",
    "    df_pos = df_coef[df_coef[\"sign\"] == \"positive\"]\n",
    "    if len(df_pos) > 0:\n",
    "        pos_feat = df_pos.iloc[0][\"feature\"]\n",
    "        save_path = os.path.join(\n",
    "            INTERP_DIR,\"woe_profils\", f\"woe_profile_positive_beta_{pos_feat}.png\"\n",
    "        )\n",
    "        print(f\"\\n Profil WoE dla cechy z dodatniƒÖ betƒÖ: {pos_feat}\")\n",
    "        plot_woe_profile(X_woe, y, pos_feat, save_path)\n",
    "    else:\n",
    "        print(\"\\n Brak cech z dodatniƒÖ betƒÖ ‚Äì nie rysujƒô osobnego profilu dla beta > 0.\")\n",
    "\n",
    "    # top-5 cech z ujemnƒÖ betƒÖ wg |beta|\n",
    "    df_neg = df_coef[df_coef[\"sign\"] == \"negative\"].head(9)\n",
    "    print(\"\\n Profile WoE dla 9 cech z najwiƒôkszym |beta| (beta < 0):\")\n",
    "    for feat in df_neg[\"feature\"]:\n",
    "        save_path = os.path.join(INTERP_DIR,\"woe_profils\", f\"woe_profile_top_negative_{feat}.png\")\n",
    "        plot_woe_profile(X_woe, y, feat, save_path)\n",
    "\n",
    "# ============================================================\n",
    "#      DIAGNOSTYKA LICZEBNO≈öCI BIN√ìW DLA PROFILI WOE\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_split():\n",
    "    \"\"\"\n",
    "    Odwzorowuje dok≈Çadnie ten sam podzia≈Ç 60/20/20,\n",
    "    kt√≥rego u≈ºywali≈õmy do trenowania modeli.\n",
    "    Zwraca X_train, y_train.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    # Val i test sƒÖ nam tutaj niepotrzebne ‚Äì patrzymy tylko na rozk≈Çad w trainie\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def load_woe_transformer():\n",
    "    \"\"\"\n",
    "    ≈Åaduje pipeline preprocessingowy dla logitu i wyciƒÖga z niego krok 'woe'.\n",
    "    \"\"\"\n",
    "    preproc_path = os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\")\n",
    "    if not os.path.exists(preproc_path):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pipeline'u WoE pod ≈õcie≈ºkƒÖ: {preproc_path}\")\n",
    "\n",
    "    preproc = joblib.load(preproc_path)\n",
    "    if \"woe\" not in preproc.named_steps:\n",
    "        raise ValueError(\"W pipeline'ie nie ma kroku o nazwie 'woe'.\")\n",
    "\n",
    "    return preproc.named_steps[\"woe\"]\n",
    "\n",
    "\n",
    "def compute_bin_table_for_feature(feature, X_train, y_train, woe_tr, min_count=50):\n",
    "    \"\"\"\n",
    "    Buduje tabelƒô:\n",
    "        bin (wg granic z WoE),\n",
    "        count_total,\n",
    "        count_good,\n",
    "        count_bad,\n",
    "        default_rate\n",
    "    i zwraca jƒÖ jako DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if feature not in woe_tr.bin_edges_:\n",
    "        raise KeyError(f\"Brak granic bin√≥w dla cechy '{feature}' w woe.bin_edges_\")\n",
    "\n",
    "    edges = np.array(woe_tr.bin_edges_[feature])\n",
    "\n",
    "    # Dzielimy X_train na biny wed≈Çug granic z WoE\n",
    "    bins = pd.cut(\n",
    "        X_train[feature],\n",
    "        bins=edges,\n",
    "        include_lowest=True,\n",
    "        right=True,\n",
    "    )\n",
    "\n",
    "    # Tabelka liczno≈õci good/bad\n",
    "    ctab = pd.crosstab(bins, y_train).rename(columns={0: \"good\", 1: \"bad\"})\n",
    "    if \"good\" not in ctab.columns:\n",
    "        ctab[\"good\"] = 0\n",
    "    if \"bad\" not in ctab.columns:\n",
    "        ctab[\"bad\"] = 0\n",
    "\n",
    "    ctab[\"total\"] = ctab[\"good\"] + ctab[\"bad\"]\n",
    "    ctab[\"default_rate\"] = ctab[\"bad\"] / ctab[\"total\"].replace(0, np.nan)\n",
    "\n",
    "    ctab = ctab.reset_index().rename(columns={X_train[feature].name: \"bin\"})\n",
    "\n",
    "    # Flaga ma≈Çej liczno≈õci\n",
    "    ctab[\"low_count_flag\"] = ctab[\"total\"] < min_count\n",
    "\n",
    "    return ctab\n",
    "\n",
    "def diagnose_bin_sizes(df_coef, n_top=5, min_count=50):\n",
    "    \"\"\"\n",
    "    Sprawdza, czy dziwne zachowanie na ko≈Ñcach profili WoE\n",
    "    mo≈ºe wynikaƒá z bardzo ma≈Çej liczno≈õci skrajnych bin√≥w.\n",
    "\n",
    "    Dla:\n",
    "      - top n_top cech wg |beta| (ujemne),\n",
    "      - wszystkich cech z beta > 0\n",
    "    zapisuje tabelki liczno≈õci do CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n Diagnostyka liczno≈õci bin√≥w WoE...\")\n",
    "\n",
    "    # 1. Pobieramy train i WoETransformera\n",
    "    X_train, y_train = get_train_split()\n",
    "    woe_tr = load_woe_transformer()\n",
    "\n",
    "    # 2. Top cechy wg |beta| (ujemne)\n",
    "    top_neg = df_coef[df_coef[\"sign\"] == \"negative\"].head(n_top)[\"feature\"].tolist()\n",
    "    pos_feats = df_coef[df_coef[\"sign\"] == \"positive\"][\"feature\"].tolist()\n",
    "\n",
    "    features_to_check = top_neg + pos_feats\n",
    "\n",
    "    all_tables = []\n",
    "\n",
    "    for feat in features_to_check:\n",
    "        try:\n",
    "            tbl = compute_bin_table_for_feature(feat, X_train, y_train, woe_tr, min_count=min_count)\n",
    "        except KeyError as e:\n",
    "            print(f\" [WARN] Pomijam {feat}: {e}\")\n",
    "            continue\n",
    "\n",
    "        tbl[\"feature\"] = feat\n",
    "        all_tables.append(tbl)\n",
    "\n",
    "        # Kr√≥tkie podsumowanie w konsoli\n",
    "        print(f\"\\n Cechy bin√≥w ‚Äì {feat}:\")\n",
    "        print(tbl[[\"bin\", \"total\", \"good\", \"bad\", \"default_rate\", \"low_count_flag\"]].to_string(index=False))\n",
    "\n",
    "        # Zapis osobnego pliku CSV dla tej cechy\n",
    "        out_path_single = os.path.join(\n",
    "            INTERP_DIR,\n",
    "            f\"woe_bin_counts_{feat}.csv\"\n",
    "        )\n",
    "        tbl.to_csv(out_path_single, index=False)\n",
    "\n",
    "    if all_tables:\n",
    "        full = pd.concat(all_tables, ignore_index=True)\n",
    "        out_path_all = os.path.join(INTERP_DIR, \"woe_bin_counts_all_checked_features.csv\")\n",
    "        full.to_csv(out_path_all, index=False)\n",
    "        print(f\"\\n Zapisano zbiorczƒÖ tabelƒô liczno≈õci bin√≥w do: {out_path_all}\")\n",
    "    else:\n",
    "        print(\"\\n Nie uda≈Ço siƒô zbudowaƒá ≈ºadnej tabeli bin√≥w ‚Äì sprawd≈∫ nazwy cech i WoE.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                RANKING CECH I CONTRIBUTION PLOT\n",
    "# ============================================================\n",
    "\n",
    "def plot_beta_importance(df_coef, top_n=9):\n",
    "    \"\"\"Wykres s≈Çupkowy top_n cech wg |beta|.\"\"\"\n",
    "    df_top = df_coef.head(top_n).iloc[::-1]  # od najmniejszej do najwiƒôkszej na osi Y\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = [\"tab:red\" if b > 0 else \"tab:green\" for b in df_top[\"beta\"]]\n",
    "    plt.barh(df_top[\"feature\"], df_top[\"beta\"], color=colors)\n",
    "    plt.axvline(0, color=\"black\", linewidth=1)\n",
    "    plt.xlabel(\"Warto≈õƒá wsp√≥≈Çczynnika Œ≤\")\n",
    "    plt.title(f\"Top {top_n} cech wg |Œ≤|\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(WYKRESY_DIR, f\"beta_importance_top{top_n}.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"\\n Zapisano wykres wa≈ºno≈õci cech: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_contribution_for_top_case(df_coef, intercept, X_woe, y, logit):\n",
    "    \"\"\"Contribution plot dla obserwacji o najwy≈ºszym PD.\"\"\"\n",
    "    proba = logit.predict_proba(X_woe)[:, 1]\n",
    "    top_idx = np.argmax(proba)\n",
    "\n",
    "    x_row = X_woe.iloc[top_idx]\n",
    "    beta = df_coef.set_index(\"feature\")[\"beta\"]\n",
    "\n",
    "    contrib = x_row * beta\n",
    "    df_contrib = contrib.to_frame(\"contribution\")\n",
    "    df_contrib[\"abs_contrib\"] = df_contrib[\"contribution\"].abs()\n",
    "    df_contrib = df_contrib.sort_values(\"abs_contrib\", ascending=False).head(15)\n",
    "    df_contrib = df_contrib.iloc[::-1]  # do barh\n",
    "\n",
    "    # logit i PD dla tej obserwacji\n",
    "    logit_val = intercept + (x_row * beta).sum()\n",
    "    pd_val = 1 / (1 + np.exp(-logit_val))\n",
    "    base_pd = 1 / (1 + np.exp(-intercept))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = [\"tab:red\" if c > 0 else \"tab:green\" for c in df_contrib[\"contribution\"]]\n",
    "    plt.barh(df_contrib.index, df_contrib[\"contribution\"], color=colors)\n",
    "    plt.axvline(0, color=\"black\", linewidth=1)\n",
    "    plt.xlabel(\"Wk≈Çad do logitu (Œ≤_j * x_j)\")\n",
    "    plt.title(\n",
    "        f\"Contribution plot ‚Äì top case (PD={pd_val:.3f}, base PD={base_pd:.3f})\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(WYKRESY_DIR, \"contribution_top_case.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\" Zapisano contribution plot: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                     PDP i ICE DLA TOP CECH\n",
    "# ============================================================\n",
    "\n",
    "def compute_pdp_ice_for_feature(X_woe, y, logit, feature,\n",
    "                                grid_size=20, ice_samples=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Liczy PDP i ICE dla pojedynczej cechy:\n",
    "      - PDP: ≈õrednie PD po zastƒÖpieniu danej cechy r√≥≈ºnymi warto≈õciami z gridu\n",
    "      - ICE: dla wybranych obserwacji ≈õledzimy PD w funkcji tej cechy\n",
    "    Zwraca (grid, pdp_values, ice_curves), gdzie:\n",
    "      - grid: warto≈õci cechy\n",
    "      - pdp_values: mean PD dla ka≈ºdego z gridu\n",
    "      - ice_curves: lista np.array o d≈Çugo≈õci grid_size (ka≈ºda to krzywa dla 1 obserwacji)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    x_vals = X_woe[feature].values\n",
    "\n",
    "    # ograniczamy siƒô do \"≈õrodka\" rozk≈Çadu\n",
    "    low, high = np.percentile(x_vals, [5, 95])\n",
    "    grid = np.linspace(low, high, grid_size)\n",
    "\n",
    "    # PDP\n",
    "    pdp_values = []\n",
    "    for v in grid:\n",
    "        X_mod = X_woe.copy()\n",
    "        X_mod[feature] = v\n",
    "        pdp_values.append(logit.predict_proba(X_mod)[:, 1].mean())\n",
    "    pdp_values = np.array(pdp_values)\n",
    "\n",
    "    # ICE\n",
    "    n_samples = min(ice_samples, len(X_woe))\n",
    "    sample_idx = rng.choice(len(X_woe), size=n_samples, replace=False)\n",
    "\n",
    "    ice_curves = []\n",
    "    for idx in sample_idx:\n",
    "        row = X_woe.iloc[idx:idx+1].copy()\n",
    "        preds = []\n",
    "        for v in grid:\n",
    "            row_mod = row.copy()\n",
    "            row_mod[feature] = v\n",
    "            preds.append(logit.predict_proba(row_mod)[:, 1][0])\n",
    "        ice_curves.append(np.array(preds))\n",
    "\n",
    "    return grid, pdp_values, ice_curves\n",
    "\n",
    "\n",
    "def plot_pdp(grid, pdp_values, feature):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(grid, pdp_values, \"-o\")\n",
    "    plt.xlabel(f\"{feature} (WoE)\")\n",
    "    plt.ylabel(\"≈örednie PD\")\n",
    "    plt.title(f\"PDP ‚Äì {feature}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(PDP_DIR, f\"pdp_{feature}.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"   ‚ûú zapisano PDP: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_ice(grid, ice_curves, feature):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for curve in ice_curves:\n",
    "        plt.plot(grid, curve, alpha=0.2, color=\"tab:blue\")\n",
    "    plt.xlabel(f\"{feature} (WoE)\")\n",
    "    plt.ylabel(\"PD\")\n",
    "    plt.title(f\"ICE ‚Äì {feature}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(ICE_DIR, f\"ice_{feature}.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"   ‚ûú zapisano ICE: {out_path}\")\n",
    "\n",
    "\n",
    "def generate_pdp_ice_for_top_features(df_coef, X_woe, y, logit, top_n=9):\n",
    "    \"\"\"PDP + ICE dla top_n cech wg |beta| (niezale≈ºnie od znaku).\"\"\"\n",
    "    df_top = df_coef.head(top_n)\n",
    "    print(f\"\\n PDP i ICE dla top {top_n} cech wg |beta|:\")\n",
    "\n",
    "    for feat in df_top[\"feature\"]:\n",
    "        print(f\"   ‚Ä¢ {feat}\")\n",
    "        grid, pdp_vals, ice_curves = compute_pdp_ice_for_feature(\n",
    "            X_woe, y, logit, feature=feat\n",
    "        )\n",
    "        plot_pdp(grid, pdp_vals, feat)\n",
    "        plot_ice(grid, ice_curves, feat)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             LOKALNA INTERPRETACJA ‚Äì 9 PRZYPADK√ìW\n",
    "# ============================================================\n",
    "\n",
    "INTERP_LOCAL_DIR = os.path.join(INTERP_DIR, \"interpretowalnosc_lokalna\")\n",
    "os.makedirs(INTERP_LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "LOGIT_PREPROC_PATH = os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\")\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def load_logit_preproc():\n",
    "    \"\"\"\n",
    "    ≈Åaduje pipeline preprocessingowy dla logitu (WoE).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(LOGIT_PREPROC_PATH):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pipeline'u logitu pod: {LOGIT_PREPROC_PATH}\")\n",
    "    return joblib.load(LOGIT_PREPROC_PATH)\n",
    "\n",
    "\n",
    "def get_data_splits_for_local():\n",
    "    \"\"\"\n",
    "    Odwzorowuje podzia≈Ç 60/20/20 u≈ºywany w projekcie.\n",
    "    Zwraca: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def transform_to_feature_df(preproc_logit, logit, X):\n",
    "    \"\"\"\n",
    "    Przepuszcza X przez preproc_logit i zwraca DataFrame\n",
    "    z kolumnami w tej samej kolejno≈õci, co feature_names_in_ modelu logit.\n",
    "    \"\"\"\n",
    "    X_tr = preproc_logit.transform(X)\n",
    "\n",
    "    if isinstance(X_tr, pd.DataFrame):\n",
    "        # upewniamy siƒô, ≈ºe kolumny sƒÖ w tej samej kolejno≈õci\n",
    "        return X_tr.loc[:, logit.feature_names_in_]\n",
    "\n",
    "    # je≈õli np. jest to ndarray:\n",
    "    return pd.DataFrame(X_tr, columns=logit.feature_names_in_)\n",
    "\n",
    "\n",
    "def select_9_cases_evenly_by_pd(X_test_tr, y_test, logit):\n",
    "    \"\"\"\n",
    "    Wybiera 9 obserwacji z testu, roz≈Ço≈ºonych r√≥wnomiernie po skali PD.\n",
    "    Korzysta z kwantyli przewidzianych PD (0.05, 0.15, ..., 0.95).\n",
    "    Zwraca listƒô indeks√≥w X_test (oryginalnych).\n",
    "    \"\"\"\n",
    "    p_test = logit.predict_proba(X_test_tr)[:, 1]\n",
    "    s = pd.Series(p_test, index=y_test.index)\n",
    "\n",
    "    quantiles = np.linspace(0.05, 0.95, 9)\n",
    "    selected_idx = []\n",
    "\n",
    "    for q in quantiles:\n",
    "        target = s.quantile(q)\n",
    "        # obserwacja, kt√≥rej PD jest najbli≈ºej wybranego kwantyla\n",
    "        idx = (s - target).abs().sort_values().index[0]\n",
    "        # je≈õli ju≈º mamy tƒô obserwacjƒô, szukamy kolejnej najbli≈ºszej\n",
    "        if idx in selected_idx:\n",
    "            for alt_idx in (s - target).abs().sort_values().index:\n",
    "                if alt_idx not in selected_idx:\n",
    "                    idx = alt_idx\n",
    "                    break\n",
    "        selected_idx.append(idx)\n",
    "\n",
    "    return selected_idx, s\n",
    "\n",
    "\n",
    "def decompose_logit_for_case(idx, x_row, y_true, beta, intercept, feature_names):\n",
    "    \"\"\"\n",
    "    Rozk≈Çada logit dla pojedynczej obserwacji na wk≈Çady cech.\n",
    "    Zwraca:\n",
    "      - df_contrib: DataFrame z wk≈Çadami dla wszystkich cech (do sortowania)\n",
    "      - meta: dict z logitem, PD i y_true\n",
    "    \"\"\"\n",
    "    x_vals = x_row.values.astype(float)\n",
    "    beta_vals = beta.astype(float)\n",
    "\n",
    "    contrib = x_vals * beta_vals\n",
    "    logit_val = intercept + contrib.sum()\n",
    "    pd_val = sigmoid(logit_val)\n",
    "\n",
    "    df_contrib = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"x_value\": x_vals,\n",
    "        \"beta\": beta_vals,\n",
    "        \"contribution\": contrib,\n",
    "    })\n",
    "    df_contrib[\"abs_contribution\"] = df_contrib[\"contribution\"].abs()\n",
    "\n",
    "    # sortujemy od najbardziej wp≈Çywowych cech\n",
    "    df_contrib = df_contrib.sort_values(\"abs_contribution\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    meta = {\n",
    "        \"index\": int(idx),\n",
    "        \"y_true\": int(y_true),\n",
    "        \"logit\": float(logit_val),\n",
    "        \"pd\": float(pd_val),\n",
    "    }\n",
    "    return df_contrib, meta\n",
    "\n",
    "\n",
    "def compute_local_decomposition_for_9_cases(logit, df_coef):\n",
    "    \"\"\"\n",
    "    G≈Ç√≥wna funkcja:\n",
    "      - ≈Çaduje preproc i dane,\n",
    "      - wybiera 9 case'√≥w roz≈Ço≈ºonych po skali PD,\n",
    "      - dla ka≈ºdego case'a rozk≈Çada logit na wk≈Çady cech,\n",
    "      - zapisuje:\n",
    "          * local_cases_meta.csv ‚Äì 9 wierszy (case_id, index, y_true, logit, pd)\n",
    "          * local_cases_top10_contributions.csv ‚Äì top 9 cech dla ka≈ºdego case'a\n",
    "    \"\"\"\n",
    "    print(\"\\n Liczƒô lokalnƒÖ interpretacjƒô (9 przypadk√≥w)...\")\n",
    "\n",
    "    preproc_logit = load_logit_preproc()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = get_data_splits_for_local()\n",
    "    X_test_tr = transform_to_feature_df(preproc_logit, logit, X_test)\n",
    "\n",
    "    beta = logit.coef_.ravel()\n",
    "    intercept = float(logit.intercept_[0])\n",
    "    feature_names = np.array(logit.feature_names_in_)\n",
    "\n",
    "    # wybieramy 9 przypadk√≥w\n",
    "    selected_idx, pd_series = select_9_cases_evenly_by_pd(X_test_tr, y_test, logit)\n",
    "    print(f\"   Wybrane indeksy testu: {selected_idx}\")\n",
    "\n",
    "    meta_rows = []\n",
    "    all_top10_rows = []\n",
    "\n",
    "    for case_id, idx in enumerate(selected_idx, start=1):\n",
    "        x_row = X_test_tr.loc[idx, :]\n",
    "        y_true = y_test.loc[idx]\n",
    "\n",
    "        df_contrib, meta = decompose_logit_for_case(\n",
    "            idx=idx,\n",
    "            x_row=x_row,\n",
    "            y_true=y_true,\n",
    "            beta=beta,\n",
    "            intercept=intercept,\n",
    "            feature_names=feature_names,\n",
    "        )\n",
    "\n",
    "        # ≈ÇƒÖczymy z globalnymi informacjami o wsp√≥≈Çczynnikach (np. abs_beta, sign)\n",
    "        df_contrib = df_contrib.merge(\n",
    "            df_coef[[\"feature\", \"abs_beta\", \"sign\"]],\n",
    "            how=\"left\",\n",
    "            on=\"feature\",\n",
    "        )\n",
    "\n",
    "        # bierzemy top 9 cech\n",
    "        df_top10 = df_contrib.head(9).copy()\n",
    "        df_top10[\"case_id\"] = case_id\n",
    "        df_top10[\"original_index\"] = int(idx)\n",
    "        df_top10[\"rank\"] = np.arange(1, len(df_top10) + 1)\n",
    "\n",
    "        all_top10_rows.append(df_top10)\n",
    "\n",
    "        meta[\"case_id\"] = case_id\n",
    "        meta[\"original_index\"] = int(idx)\n",
    "        meta_rows.append(meta)\n",
    "\n",
    "    # zapis meta\n",
    "    df_meta = pd.DataFrame(meta_rows)[\n",
    "        [\"case_id\", \"original_index\", \"y_true\", \"logit\", \"pd\"]\n",
    "    ]\n",
    "    meta_path = os.path.join(INTERP_LOCAL_DIR, \"local_cases_meta.csv\")\n",
    "    df_meta.to_csv(meta_path, index=False)\n",
    "    print(f\" Zapisano podsumowanie 9 przypadk√≥w ‚Üí {meta_path}\")\n",
    "\n",
    "    # zapis top10 contributions (long format)\n",
    "    df_all_top10 = pd.concat(all_top10_rows, ignore_index=True)\n",
    "    contrib_path = os.path.join(INTERP_LOCAL_DIR, \"local_cases_top10_contributions.csv\")\n",
    "    df_all_top10.to_csv(contrib_path, index=False)\n",
    "    print(f\" Zapisano top 9 wk≈Çad√≥w cech dla 9 przypadk√≥w ‚Üí {contrib_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                           MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\" ≈Åadowanie modelu logit (WoE)...\")\n",
    "    logit = load_logit_model()\n",
    "    preproc_logit = load_logit_preprocessor()\n",
    "\n",
    "    print(\" Ekstrakcja wsp√≥≈Çczynnik√≥w...\")\n",
    "    df_coef, intercept = extract_coefficients(logit)\n",
    "\n",
    "    summarize_signs(df_coef, intercept)\n",
    "    save_coefficients(df_coef)\n",
    "\n",
    "    print(\"\\nTop 9 cech wg |beta|:\")\n",
    "    print(df_coef.head(9).to_string(index=False))\n",
    "\n",
    "    print(\"\\n Przygotowywanie danych (WoE)...\")\n",
    "    X_woe, y = load_and_prepare_data(preproc_logit, logit)\n",
    "\n",
    "    # ---------- Profile WoE ----------\n",
    "    generate_woe_profiles(df_coef, X_woe, y)\n",
    "\n",
    "    # ---------- Ranking cech ----------\n",
    "    plot_beta_importance(df_coef, top_n=9)\n",
    "\n",
    "    # ---------- Contribution plot ----------\n",
    "    plot_contribution_for_top_case(df_coef, intercept, X_woe, y, logit)\n",
    "\n",
    "    # ---------- PDP + ICE ----------\n",
    "    generate_pdp_ice_for_top_features(df_coef, X_woe, y, logit, top_n=9)\n",
    "\n",
    "    print(\"\\n Zako≈Ñczono generowanie wykres√≥w interpretowalno≈õci logitu.\")\n",
    "    \n",
    "    diagnose_bin_sizes(df_coef, n_top=9, min_count=50)\n",
    "    \n",
    "    # Lokalna interpretacja ‚Äì 5 obserwacji\n",
    "    compute_local_decomposition_for_9_cases(logit, df_coef)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpretowalnosc_logit/interpretowalnosc_lokalna/wykresy_interpretacji_lokalnej.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#           KONFIGURACJA ≈öCIE≈ªEK I FOLDER√ìW WYJ≈öCIOWYCH\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # .../interpretowalnosc_lokalna\n",
    "\n",
    "META_PATH = os.path.join(BASE_DIR, \"local_cases_meta.csv\")\n",
    "CONTRIB_PATH = os.path.join(BASE_DIR, \"local_cases_top10_contributions.csv\")\n",
    "\n",
    "PLOTS_CASE_DIR = os.path.join(BASE_DIR, \"wykresy_case\")\n",
    "PLOTS_GRID_DIR = os.path.join(BASE_DIR, \"wykresy_zbiorcze\")\n",
    "\n",
    "os.makedirs(PLOTS_CASE_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_GRID_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                Wczytanie danych lokalnych\n",
    "# ============================================================\n",
    "\n",
    "def load_local_data():\n",
    "    if not os.path.exists(META_PATH):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pliku meta: {META_PATH}\")\n",
    "    if not os.path.exists(CONTRIB_PATH):\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pliku z wk≈Çadami: {CONTRIB_PATH}\")\n",
    "\n",
    "    df_meta = pd.read_csv(META_PATH)\n",
    "    df_contrib = pd.read_csv(CONTRIB_PATH)\n",
    "\n",
    "    # Upewniamy siƒô, ≈ºe case_id jest int\n",
    "    df_meta[\"case_id\"] = df_meta[\"case_id\"].astype(int)\n",
    "    df_contrib[\"case_id\"] = df_contrib[\"case_id\"].astype(int)\n",
    "\n",
    "    return df_meta, df_contrib\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#         Wykresy pojedyncze ‚Äì top10 wk≈Çad√≥w dla case\n",
    "# ============================================================\n",
    "\n",
    "def plot_single_case_bar(df_case, meta_row, save_path):\n",
    "    \"\"\"\n",
    "    Rysuje wykres s≈Çupkowy wk≈Çad√≥w cech do logitu dla pojedynczego case'a.\n",
    "    df_case ‚Äì wiersze dla jednego case_id (top 9 cech),\n",
    "    meta_row ‚Äì Series z informacjami: case_id, original_index, y_true, logit, pd\n",
    "    \"\"\"\n",
    "    # sortujemy tak, aby najbardziej wp≈Çywowe cechy by≈Çy na g√≥rze\n",
    "    df_plot = df_case.sort_values(\"abs_contribution\", ascending=True).copy()\n",
    "\n",
    "    features = df_plot[\"feature\"]\n",
    "    contrib = df_plot[\"contribution\"]\n",
    "\n",
    "    # kolor: czerwony dla dodatnich wk≈Çad√≥w, niebieski dla ujemnych\n",
    "    colors = np.where(contrib >= 0, \"#d62728\", \"#1f77b4\")  # red / blue\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(features, contrib, color=colors)\n",
    "    plt.axvline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "    case_id = int(meta_row[\"case_id\"])\n",
    "    idx = int(meta_row[\"original_index\"])\n",
    "    y_true = int(meta_row[\"y_true\"])\n",
    "    logit_val = meta_row[\"logit\"]\n",
    "    pd_val = meta_row[\"pd\"]\n",
    "\n",
    "    plt.title(\n",
    "        f\"Case {case_id} (idx={idx}, y={y_true})\\n\"\n",
    "        f\"logit={logit_val:.3f}, PD={pd_val:.3%}\"\n",
    "    )\n",
    "    plt.xlabel(\"Wk≈Çad do logitu (beta * x)\")\n",
    "    plt.ylabel(\"Cecha\")\n",
    "    plt.grid(axis=\"x\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_per_case_plots(df_meta, df_contrib):\n",
    "    \"\"\"\n",
    "    Generuje po jednym wykresie dla ka≈ºdego z 9 case'√≥w (top 9 cech).\n",
    "    \"\"\"\n",
    "    for _, meta_row in df_meta.sort_values(\"case_id\").iterrows():\n",
    "        case_id = int(meta_row[\"case_id\"])\n",
    "        idx = int(meta_row[\"original_index\"])\n",
    "\n",
    "        df_case = df_contrib[df_contrib[\"case_id\"] == case_id].copy()\n",
    "\n",
    "        fname = f\"case_{case_id}_idx_{idx}_top9_contributions.png\"\n",
    "        save_path = os.path.join(PLOTS_CASE_DIR, fname)\n",
    "\n",
    "        plot_single_case_bar(df_case, meta_row, save_path)\n",
    "        print(f\"üíæ Zapisano wykres dla case {case_id} ‚Üí {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#     Zbiorczy wykres 3√ó3 ‚Äì 9 case'√≥w, kolor = gradient po PD\n",
    "# ============================================================\n",
    "\n",
    "def plot_grid_cases(df_meta, df_contrib, n_per_case=9):\n",
    "    \"\"\"\n",
    "    Tworzy zbiorczy wykres 3x3:\n",
    "      - ka≈ºdy subplot to top 10 wk≈Çad√≥w dla danego case'a,\n",
    "      - kolor s≈Çupk√≥w jest kolorem z mapy barw zale≈ºnym od PD (gradient).\n",
    "    \"\"\"\n",
    "    # przygotowanie mapy kolor√≥w po PD\n",
    "    pd_vals = df_meta[\"pd\"].values\n",
    "    pd_min, pd_max = pd_vals.min(), pd_vals.max()\n",
    "    norm = plt.Normalize(pd_min, pd_max)\n",
    "    cmap = plt.cm.get_cmap(\"viridis\")\n",
    "\n",
    "    cases_sorted = df_meta.sort_values(\"case_id\").reset_index(drop=True)\n",
    "    n_cases = len(cases_sorted)\n",
    "\n",
    "    n_rows, n_cols = 3, 3  # zak≈Çadamy 9 case'√≥w\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, (_, meta_row) in enumerate(cases_sorted.iterrows()):\n",
    "        ax = axes[i]\n",
    "        case_id = int(meta_row[\"case_id\"])\n",
    "        idx = int(meta_row[\"original_index\"])\n",
    "        y_true = int(meta_row[\"y_true\"])\n",
    "        logit_val = meta_row[\"logit\"]\n",
    "        pd_val = meta_row[\"pd\"]\n",
    "\n",
    "        df_case = df_contrib[df_contrib[\"case_id\"] == case_id].copy()\n",
    "        df_case = df_case.sort_values(\"abs_contribution\", ascending=True).tail(n_per_case)\n",
    "\n",
    "        features = df_case[\"feature\"]\n",
    "        contrib = df_case[\"contribution\"]\n",
    "\n",
    "        # kolor ca≈Çego case'a ‚Äì jeden kolor z gradientu po PD\n",
    "        color_case = cmap(norm(pd_val))\n",
    "\n",
    "        ax.barh(features, contrib, color=color_case)\n",
    "        ax.axvline(0, color=\"black\", linewidth=0.8)\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Case {case_id} (idx={idx}, y={y_true})\\n\"\n",
    "            f\"PD={pd_val:.2%}\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Wk≈Çad do logitu\")\n",
    "        ax.set_ylabel(\"Cecha\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # je≈õli case'√≥w mniej ni≈º 9, ukrywamy puste osie\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    # dodajemy pasek koloru opisujƒÖcy PD\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label(\"PD (prawdopodobie≈Ñstwo defaultu)\")\n",
    "\n",
    "    fig.suptitle(\"Lokalna interpretacja ‚Äì top 10 wk≈Çad√≥w (9 przypadk√≥w)\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0.03, 0.03, 0.9, 0.95])\n",
    "\n",
    "    out_path = os.path.join(PLOTS_GRID_DIR, \"grid_3x3_cases_top10_contributions.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"üíæ Zapisano zbiorczy wykres 3x3 ‚Üí {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                                MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"üìÇ Wczytywanie danych lokalnej interpretacji...\")\n",
    "    df_meta, df_contrib = load_local_data()\n",
    "\n",
    "    print(\"üñºÔ∏è Rysujƒô wykresy pojedynczych case'√≥w...\")\n",
    "    generate_per_case_plots(df_meta, df_contrib)\n",
    "\n",
    "    print(\"üñºÔ∏è Rysujƒô zbiorczy wykres 3x3 z gradientem po PD...\")\n",
    "    plot_grid_cases(df_meta, df_contrib, n_per_case=9)\n",
    "\n",
    "    print(\"‚úÖ Gotowe ‚Äì lokalna interpretacja zwizualizowana.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c54dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modele_nieinterpretowalne.py\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "import re\n",
    "import ast\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import ks_2samp, randint, uniform\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# KONFIGURACJA SCIEZEK\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n",
    "\n",
    "# ‚úÖ DODAJ TE LINIE - dodaj folder EDA do sys.path\n",
    "EDA_DIR = os.path.join(PROJECT_ROOT, \"EDA\")\n",
    "if EDA_DIR not in sys.path:\n",
    "    sys.path.append(EDA_DIR)\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(PROJECT_ROOT, \"EDA\", \"preprocesing_pipelines\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models_blackbox\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"blackbox_results\")\n",
    "SHAP_DIR = os.path.join(RESULTS_DIR, \"shap_plots\")\n",
    "LIME_DIR = os.path.join(RESULTS_DIR, \"lime_explanations\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(SHAP_DIR, exist_ok=True)\n",
    "os.makedirs(LIME_DIR, exist_ok=True)\n",
    "def calculate_ks_statistic(y_true, y_pred_proba):\n",
    "    data = pd.DataFrame({\"y\": y_true, \"p\": y_pred_proba}).sort_values(\"p\")\n",
    "    pos_probs = data.loc[data[\"y\"] == 1, \"p\"]\n",
    "    neg_probs = data.loc[data[\"y\"] == 0, \"p\"]\n",
    "    if len(pos_probs) == 0 or len(neg_probs) == 0:\n",
    "        return np.nan\n",
    "    ks_stat, _ = ks_2samp(pos_probs, neg_probs)\n",
    "    return ks_stat\n",
    "\n",
    "def create_xgboost_grid():\n",
    "    model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "    )\n",
    "    param_distributions = {\n",
    "        \"n_estimators\": randint(100, 500),\n",
    "        \"max_depth\": randint(3, 10),\n",
    "        \"learning_rate\": uniform(0.01, 0.2),\n",
    "        \"min_child_weight\": randint(1, 10),\n",
    "        \"subsample\": uniform(0.6, 0.4),\n",
    "        \"colsample_bytree\": uniform(0.6, 0.4),\n",
    "        \"reg_alpha\": uniform(0, 1),\n",
    "        \"reg_lambda\": uniform(0, 2),\n",
    "        \"scale_pos_weight\": [1, 2, 3],\n",
    "    }\n",
    "    return model, param_distributions\n",
    "\n",
    "def create_lightgbm_grid():\n",
    "    model = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    param_distributions = {\n",
    "        \"n_estimators\": randint(100, 500),\n",
    "        \"num_leaves\": randint(20, 100),\n",
    "        \"max_depth\": randint(3, 10),\n",
    "        \"learning_rate\": uniform(0.01, 0.2),\n",
    "        \"min_data_in_leaf\": randint(20, 200),\n",
    "        \"feature_fraction\": uniform(0.6, 0.4),\n",
    "        \"bagging_fraction\": uniform(0.6, 0.4),\n",
    "        \"bagging_freq\": [5],\n",
    "        \"lambda_l1\": uniform(0, 1),\n",
    "        \"lambda_l2\": uniform(0, 2),\n",
    "        \"scale_pos_weight\": [1, 2, 3],\n",
    "    }\n",
    "    return model, param_distributions\n",
    "\n",
    "def create_mlp_grid():\n",
    "    model = MLPClassifier(\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "    )\n",
    "    param_distributions = {\n",
    "        \"hidden_layer_sizes\": [(50,), (100,), (150,), (50, 25), (100, 50), (150, 75)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": uniform(0.0001, 0.01),\n",
    "        \"learning_rate_init\": uniform(0.001, 0.01),\n",
    "        \"batch_size\": [32, 64, 128],\n",
    "    }\n",
    "    return model, param_distributions\n",
    "\n",
    "def evaluate_model(model, X, y, model_name=\"Model\", dataset_name=\"val\"):\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"roc_auc\": roc_auc_score(y, y_pred_proba),\n",
    "        \"pr_auc\": average_precision_score(y, y_pred_proba),\n",
    "        \"ks\": calculate_ks_statistic(y, y_pred_proba),\n",
    "        \"log_loss\": log_loss(y, y_pred_proba),\n",
    "        \"brier\": brier_score_loss(y, y_pred_proba),\n",
    "    }\n",
    "\n",
    "def print_evaluation_table(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" WYNIKI MODELI BLACK-BOX (VAL + TEST)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    return df\n",
    "\n",
    "def train_with_randomized_search(model, param_distributions, X_train, y_train, model_name=\"Model\", n_iter=50, cv=5):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Tuning {model_name} z RandomizedSearchCV\")\n",
    "    print(\"=\" * 80)\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "    print(\"\\nNajlepsze parametry:\")\n",
    "    print(rs.best_params_)\n",
    "    print(f\"Najlepszy ROC-AUC CV: {rs.best_score_:.4f}\")\n",
    "    cv_results = pd.DataFrame(rs.cv_results_)\n",
    "    best_idx = rs.best_index_\n",
    "    train_score = cv_results.loc[best_idx, \"mean_train_score\"]\n",
    "    val_score = cv_results.loc[best_idx, \"mean_test_score\"]\n",
    "    print(f\"Train ROC-AUC: {train_score:.4f}, Val ROC-AUC: {val_score:.4f}\")\n",
    "    print(f\"Overfitting gap: {train_score - val_score:.4f}\")\n",
    "    return rs.best_estimator_, rs\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_shap_explanations(model, X_train, X_test, feature_names, model_name):\n",
    "    print(f\"\\nGenerowanie wyjasnien SHAP dla {model_name}...\")\n",
    "\n",
    "    # Poprawka base_score dla XGBoost, je≈õli potrzebna (je≈õli u≈ºywasz XGBoost)\n",
    "    if hasattr(model, \"get_booster\"):\n",
    "        booster = model.get_booster()\n",
    "        base_score = booster.attr(\"base_score\")\n",
    "        if base_score is not None:\n",
    "            if isinstance(base_score, str):\n",
    "                try:\n",
    "                    base_score = ast.literal_eval(base_score)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if isinstance(base_score, (list, tuple, np.ndarray)):\n",
    "                base_score = base_score[0]\n",
    "            base_score = float(base_score)\n",
    "            booster.set_param(\"base_score\", base_score)\n",
    "\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if hasattr(model, \"get_booster\") or hasattr(model, \"booster_\"):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    else:\n",
    "        background = shap.sample(X_train, 100)\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test[:500])\n",
    "\n",
    "    # Obs≈Çuga r√≥≈ºnych format√≥w shap_values (lista po klasach lub ndarray 3D)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values_class = shap_values[1]  # wybierz SHAP dla klasy pozytywnej (indeks 1)\n",
    "    elif len(shap_values.shape) == 3:\n",
    "        shap_values_class = shap_values[..., 1]  # wybierz SHAP dla klasy pozytywnej\n",
    "    else:\n",
    "        shap_values_class = shap_values\n",
    "\n",
    "    mean_abs_shap = np.abs(shap_values_class).mean(axis=0)\n",
    "    top_features_idx = np.argsort(mean_abs_shap)[-3:][::-1]\n",
    "\n",
    "    # wykres podsumowujƒÖcy (bar)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values_class, X_test[:500], feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SHAP_DIR, f\"{model_name}_summary_bar.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # wykres typu beeswarm\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_class, X_test[:500], feature_names=feature_names, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SHAP_DIR, f\"{model_name}_beeswarm.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # wykresy zale≈ºno≈õci dla 3 najwa≈ºniejszych cech\n",
    "    for idx in top_features_idx:\n",
    "        shap.dependence_plot(idx, shap_values_class, X_test[:500], feature_names=feature_names, interaction_index=None, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SHAP_DIR, f\"{model_name}_dependence_{feature_names[idx]}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Wykresy SHAP zapisane w {SHAP_DIR}\")\n",
    "\n",
    "    return explainer, shap_values_class\n",
    "\n",
    "\n",
    "def generate_lime_explanations(model, X_train, X_test, y_test, feature_names, model_name, n_instances=5):\n",
    "    print(f\"\\nGenerowanie wyjasnien LIME dla {model_name}...\")\n",
    "    \n",
    "    # Upewnij siƒô, ≈ºe dane to dense numpy array\n",
    "    if hasattr(X_train, \"toarray\"):\n",
    "        X_train = X_train.toarray()\n",
    "    if hasattr(X_test, \"toarray\"):\n",
    "        X_test = X_test.toarray()\n",
    "        \n",
    "    if hasattr(X_train, \"values\"):\n",
    "        X_train = X_train.values\n",
    "    if hasattr(X_test, \"values\"):\n",
    "        X_test = X_test.values\n",
    "        \n",
    "    # Upewnij siƒô, ≈ºe feature_names to lista\n",
    "    feature_names = list(feature_names)\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train,\n",
    "        feature_names=feature_names,\n",
    "        class_names=[\"No Default\", \"Default\"],\n",
    "        mode=\"classification\",\n",
    "        random_state=42,\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    tp_idx = np.where((y_test == 1) & (y_pred == 1))[0]\n",
    "    tn_idx = np.where((y_test == 0) & (y_pred == 0))[0]\n",
    "    fp_idx = np.where((y_test == 0) & (y_pred == 1))[0]\n",
    "    fn_idx = np.where((y_test == 1) & (y_pred == 0))[0]\n",
    "\n",
    "    instances = []\n",
    "    labels = []\n",
    "\n",
    "    if len(tp_idx) > 0:\n",
    "        instances.append(tp_idx[0])\n",
    "        labels.append(\"True_Positive\")\n",
    "    if len(tn_idx) > 0:\n",
    "        instances.append(tn_idx[0])\n",
    "        labels.append(\"True_Negative\")\n",
    "    if len(fp_idx) > 0:\n",
    "        instances.append(fp_idx[0])\n",
    "        labels.append(\"False_Positive\")\n",
    "    if len(fn_idx) > 0:\n",
    "        instances.append(fn_idx[0])\n",
    "        labels.append(\"False_Negative\")\n",
    "\n",
    "    lime_explanations = []\n",
    "\n",
    "    for i, (idx, label) in enumerate(zip(instances, labels)):\n",
    "        # explain_instance wymaga pojedynczej instancji jako 1D array\n",
    "        exp = explainer.explain_instance(X_test[idx], model.predict_proba, num_features=10)\n",
    "        \n",
    "        exp.save_to_file(os.path.join(LIME_DIR, f\"{model_name}_{label}_instance_{idx}.html\"))\n",
    "        \n",
    "        fig = exp.as_pyplot_figure()\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(os.path.join(LIME_DIR, f\"{model_name}_{label}_instance_{idx}.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        lime_explanations.append({\n",
    "            \"instance_idx\": idx,\n",
    "            \"label\": label,\n",
    "            \"true_class\": y_test[idx],\n",
    "            \"predicted_class\": y_pred[idx],\n",
    "            \"predicted_proba\": y_pred_proba[idx],\n",
    "            \"explanation\": exp.as_list(),\n",
    "        })\n",
    "\n",
    "    lime_df = pd.DataFrame(lime_explanations)\n",
    "    lime_df.to_csv(os.path.join(LIME_DIR, f\"{model_name}_lime_explanations.csv\"), index=False)\n",
    "    print(f\"Wyjasnienia LIME zapisane w {LIME_DIR}\")\n",
    "\n",
    "    return lime_explanations\n",
    "\n",
    "def main():\n",
    "    print(\"Wczytywanie danych...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(\"\\nLadowanie pipeline preprocessingu dla modeli nieinterpretowalnych...\")\n",
    "    preproc = joblib.load(os.path.join(PREPROC_DIR, \"preprocessing_blackbox.pkl\"))\n",
    "    print(\"\\nTransformacja danych...\")\n",
    "    X_train_proc = preproc.transform(X_train)\n",
    "    X_val_proc = preproc.transform(X_val)\n",
    "    X_test_proc = preproc.transform(X_test)\n",
    "    feature_names = preproc.get_feature_names_out()\n",
    "    \n",
    "    xgb_model, xgb_grid = create_xgboost_grid()\n",
    "    best_xgb, rs_xgb = train_with_randomized_search(xgb_model, xgb_grid, X_train_proc, y_train, \"XGBoost\", n_iter=50, cv=5)\n",
    "    \n",
    "    lgbm_model, lgbm_grid = create_lightgbm_grid()\n",
    "    best_lgbm, rs_lgbm = train_with_randomized_search(lgbm_model, lgbm_grid, X_train_proc, y_train, \"LightGBM\", n_iter=50, cv=5)\n",
    "    \n",
    "    mlp_model, mlp_grid = create_mlp_grid()\n",
    "    best_mlp, rs_mlp = train_with_randomized_search(mlp_model, mlp_grid, X_train_proc, y_train, \"MLP\", n_iter=30, cv=5)\n",
    "    \n",
    "    results = []\n",
    "    results.append(evaluate_model(best_xgb, X_val_proc, y_val, \"XGBoost\", \"val\"))\n",
    "    results.append(evaluate_model(best_xgb, X_test_proc, y_test, \"XGBoost\", \"test\"))\n",
    "    results.append(evaluate_model(best_lgbm, X_val_proc, y_val, \"LightGBM\", \"val\"))\n",
    "    results.append(evaluate_model(best_lgbm, X_test_proc, y_test, \"LightGBM\", \"test\"))\n",
    "    results.append(evaluate_model(best_mlp, X_val_proc, y_val, \"MLP\", \"val\"))\n",
    "    results.append(evaluate_model(best_mlp, X_test_proc, y_test, \"MLP\", \"test\"))\n",
    "    df_results = print_evaluation_table(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENEROWANIE WYSJASNIEN SHAP\")\n",
    "    print(\"=\" * 80)\n",
    "    shap_xgb = generate_shap_explanations(best_xgb, X_train_proc, X_test_proc, feature_names, \"XGBoost\")\n",
    "    shap_lgbm = generate_shap_explanations(best_lgbm, X_train_proc, X_test_proc, feature_names, \"LightGBM\")\n",
    "    shap_mlp = generate_shap_explanations(best_mlp, X_train_proc, X_test_proc, feature_names, \"MLP\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENEROWANIE WYJASNIEN LIME\")\n",
    "    print(\"=\" * 80)\n",
    "    lime_xgb = generate_lime_explanations(best_xgb, X_train_proc, X_test_proc, y_test.values, feature_names, \"XGBoost\")\n",
    "    lime_lgbm = generate_lime_explanations(best_lgbm, X_train_proc, X_test_proc, y_test.values, feature_names, \"LightGBM\")\n",
    "    lime_mlp = generate_lime_explanations(best_mlp, X_train_proc, X_test_proc, y_test.values, feature_names, \"MLP\")\n",
    "    \n",
    "    print(\"\\nZapisywanie modeli i wynikow...\")\n",
    "    joblib.dump(best_xgb, os.path.join(MODELS_DIR, \"best_xgboost.pkl\"))\n",
    "    joblib.dump(best_lgbm, os.path.join(MODELS_DIR, \"best_lightgbm.pkl\"))\n",
    "    joblib.dump(best_mlp, os.path.join(MODELS_DIR, \"best_mlp.pkl\"))\n",
    "    df_results.to_csv(os.path.join(RESULTS_DIR, \"blackbox_evaluation_results.csv\"), index=False)\n",
    "    pd.DataFrame(rs_xgb.cv_results_).to_csv(os.path.join(RESULTS_DIR, \"grid_results_xgboost.csv\"), index=False)\n",
    "    pd.DataFrame(rs_lgbm.cv_results_).to_csv(os.path.join(RESULTS_DIR, \"grid_results_lightgbm.csv\"), index=False)\n",
    "    pd.DataFrame(rs_mlp.cv_results_).to_csv(os.path.join(RESULTS_DIR, \"grid_results_mlp.csv\"), index=False)\n",
    "    \n",
    "    print(\"Zapisano wszystkie modele black-box i wyniki.\")\n",
    "    print(f\"\\nWyniki w: {RESULTS_DIR}\")\n",
    "    print(f\"Wykresy SHAP w: {SHAP_DIR}\")\n",
    "    print(f\"Wyjasnienia LIME w: {LIME_DIR}\")\n",
    "    return best_xgb, best_lgbm, best_mlp, df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be892b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kalibracja.py\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. KONFIGURACJA ≈öCIE≈ªEK\n",
    "# =============================================================================\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\n",
    "\n",
    "# [FIX] Dodajemy folder EDA do sys.path, aby joblib widzia≈Ç definicje klas\n",
    "EDA_DIR = os.path.join(PROJECT_ROOT, \"EDA\")\n",
    "if EDA_DIR not in sys.path:\n",
    "    sys.path.insert(0, EDA_DIR)\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "PREPROC_DIR = os.path.join(PROJECT_ROOT, \"EDA\", \"preprocesing_pipelines\")\n",
    "MODELS_INTERP_DIR = os.path.join(PROJECT_ROOT, \"Modele_interpretowalne\", \"models\")\n",
    "MODELS_BLACKBOX_DIR = os.path.join(PROJECT_ROOT, \"Modele_nieinterpretowalne\", \"models_blackbox\")\n",
    "\n",
    "OUTPUT_DIR = CURRENT_DIR\n",
    "IMG_DIR = os.path.join(OUTPUT_DIR, \"plots_separate\") # Nowy folder na oddzielne wykresy\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_MEAN_PD = 0.04\n",
    "\n",
    "# =============================================================================\n",
    "# 2. KLASY KALIBRATOR√ìW\n",
    "# =============================================================================\n",
    "\n",
    "class BetaCalibration(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.lr = LogisticRegression(C=999999999, solver='lbfgs')\n",
    "\n",
    "    def fit(self, X_probs, y):\n",
    "        eps = 1e-15\n",
    "        p = np.clip(X_probs, eps, 1 - eps)\n",
    "        l_p = np.log(p)\n",
    "        l_1_p = -np.log(1 - p)\n",
    "        X_trans = np.column_stack([l_p, l_1_p])\n",
    "        self.lr.fit(X_trans, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X_probs):\n",
    "        eps = 1e-15\n",
    "        p = np.clip(X_probs, eps, 1 - eps)\n",
    "        l_p = np.log(p)\n",
    "        l_1_p = -np.log(1 - p)\n",
    "        X_trans = np.column_stack([l_p, l_1_p])\n",
    "        return self.lr.predict_proba(X_trans)\n",
    "\n",
    "\n",
    "class CalibrationInTheLarge(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, target_mean=0.04):\n",
    "        self.target_mean = target_mean\n",
    "        self.delta = 0.0\n",
    "\n",
    "    def fit(self, X_probs, y=None):\n",
    "        eps = 1e-15\n",
    "        p = np.clip(X_probs, eps, 1 - eps)\n",
    "        logits = np.log(p / (1 - p))\n",
    "\n",
    "        def objective(delta):\n",
    "            shifted_logits = logits + delta\n",
    "            shifted_probs = 1 / (1 + np.exp(-shifted_logits))\n",
    "            return np.mean(shifted_probs) - self.target_mean\n",
    "\n",
    "        try:\n",
    "            self.delta = brentq(objective, -10, 10)\n",
    "        except ValueError:\n",
    "            self.delta = 0.0\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X_probs):\n",
    "        eps = 1e-15\n",
    "        p = np.clip(X_probs, eps, 1 - eps)\n",
    "        logits = np.log(p / (1 - p))\n",
    "        shifted_logits = logits + self.delta\n",
    "        new_probs = 1 / (1 + np.exp(-shifted_logits))\n",
    "        return np.column_stack([1 - new_probs, new_probs])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FUNKCJE POMOCNICZE - METRYKI I WYKRESY (ZMODYFIKOWANE)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(y_true, y_prob, model_name=\"Model\"):\n",
    "    n = len(y_true)\n",
    "    base_prob = np.mean(y_true)\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    n_bins = 10\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bins) - 1\n",
    "    \n",
    "    reliability = 0.0\n",
    "    resolution = 0.0\n",
    "    ece = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        count = np.sum(mask)\n",
    "        if count > 0:\n",
    "            prob_avg = np.mean(y_prob[mask])\n",
    "            true_avg = np.mean(y_true[mask])\n",
    "            reliability += count * (prob_avg - true_avg)**2\n",
    "            resolution += count * (true_avg - base_prob)**2\n",
    "            ece += np.abs(prob_avg - true_avg) * (count / n)\n",
    "            \n",
    "    reliability /= n\n",
    "    resolution /= n\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame({'y': y_true, 'p': y_prob})\n",
    "        df['bucket'] = pd.qcut(df['p'], n_bins, duplicates='drop')\n",
    "        ace = df.groupby('bucket').apply(lambda x: np.abs(x['p'].mean() - x['y'].mean())).mean()\n",
    "    except:\n",
    "        ace = np.nan\n",
    "\n",
    "    return {\n",
    "        \"Method\": model_name,\n",
    "        \"Avg_PD\": np.mean(y_prob),\n",
    "        \"ECE\": ece,\n",
    "        \"ACE\": ace,\n",
    "        \"Brier\": brier,\n",
    "        \"Rel\": reliability,\n",
    "        \"Res\": resolution\n",
    "    }\n",
    "\n",
    "def plot_single_reliability(y_true, y_prob, title, filename):\n",
    "    \"\"\"Generuje wykres reliability z histogramem na drugiej osi Y.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    # --- O≈ö LEWA (Reliability: 0.0 - 1.0) ---\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfect\", alpha=0.6)\n",
    "    \n",
    "    frac_pos, mean_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    ax1.plot(mean_pred, frac_pos, \"s-\", label=\"Model\", color='navy', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax1.set_ylabel(\"Fraction of Positives (Reliability)\", color='navy')\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.tick_params(axis='y', labelcolor='navy')\n",
    "    ax1.set_xlabel(\"Mean Predicted Probability\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- O≈ö PRAWA (Histogram: Liczebno≈õƒá) ---\n",
    "    ax2 = ax1.twinx()  # Druga o≈õ wsp√≥≈ÇdzielƒÖca X\n",
    "    \n",
    "    # Rysujemy histogram z przezroczysto≈õciƒÖ\n",
    "    ax2.hist(y_prob, range=(0, 1), bins=10, histtype=\"stepfilled\", \n",
    "             color=\"gray\", alpha=0.2, label=\"Distribution\")\n",
    "    \n",
    "    ax2.set_ylabel(\"Count (Histogram)\", color=\"gray\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"gray\")\n",
    "    \n",
    "    # Legenda - ≈ÇƒÖczymy wpisy z obu osi\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\")\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    save_path = os.path.join(IMG_DIR, filename)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_single_histogram(y_prob, title, filename):\n",
    "    \"\"\"Generuje histogram dla pojedynczej serii danych.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.hist(y_prob, bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', label=\"PD Distribution\", density=True)\n",
    "    \n",
    "    # Target line\n",
    "    plt.axvline(TARGET_MEAN_PD, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Target {TARGET_MEAN_PD}')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Probability (PD)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    save_path = os.path.join(IMG_DIR, filename)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def find_file(directory, pattern):\n",
    "    if not os.path.exists(directory): return None\n",
    "    files = os.listdir(directory)\n",
    "    for f in files:\n",
    "        if pattern.lower() in f.lower() and f.endswith('.pkl'):\n",
    "            return os.path.join(directory, f)\n",
    "    pkls = [f for f in files if f.endswith('.pkl')]\n",
    "    if pkls: return os.path.join(directory, pkls[0])\n",
    "    return None\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\">>> [1/6] Wczytywanie danych...\")\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"Brak pliku: {DATA_PATH}\")\n",
    "        \n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    print(\">>> [2/6] Wczytywanie Preprocessingu...\")\n",
    "    \n",
    "    # Logit\n",
    "    path_pre_logit = os.path.join(PREPROC_DIR, \"preprocessing_logit_woe.pkl\")\n",
    "    if os.path.exists(path_pre_logit):\n",
    "        try:\n",
    "            pre_logit = joblib.load(path_pre_logit)\n",
    "            X_val_logit = pre_logit.transform(X_val)\n",
    "            X_test_logit = pre_logit.transform(X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"![ERROR] Logit Preproc: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        X_val_logit, X_test_logit = X_val, X_test\n",
    "\n",
    "    # Blackbox\n",
    "    path_pre_bb = os.path.join(PREPROC_DIR, \"preprocessing_blackbox.pkl\")\n",
    "    if os.path.exists(path_pre_bb):\n",
    "        try:\n",
    "            pre_bb = joblib.load(path_pre_bb)\n",
    "            X_val_bb = pre_bb.transform(X_val)\n",
    "            X_test_bb = pre_bb.transform(X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"![ERROR] Blackbox Preproc: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        X_val_bb, X_test_bb = X_val, X_test\n",
    "\n",
    "    print(\">>> [3/6] Wczytywanie Modeli...\")\n",
    "    \n",
    "    # Logit\n",
    "    path_logit = find_file(MODELS_INTERP_DIR, \"logit\") or find_file(MODELS_INTERP_DIR, \"logistic\")\n",
    "    if path_logit:\n",
    "        model_logit = joblib.load(path_logit)\n",
    "        p_val_logit = model_logit.predict_proba(X_val_logit)[:, 1]\n",
    "        p_test_logit = model_logit.predict_proba(X_test_logit)[:, 1]\n",
    "    else:\n",
    "        print(\"![ERROR] Brak modelu Logit.\")\n",
    "        p_val_logit, p_test_logit = np.zeros(len(y_val)), np.zeros(len(y_test))\n",
    "\n",
    "    # XGBoost\n",
    "    path_xgb = find_file(MODELS_BLACKBOX_DIR, \"xgboost\") or find_file(MODELS_BLACKBOX_DIR, \"boost\")\n",
    "    if path_xgb:\n",
    "        model_xgb = joblib.load(path_xgb)\n",
    "        try:\n",
    "            p_val_xgb = model_xgb.predict_proba(X_val_bb)[:, 1]\n",
    "            p_test_xgb = model_xgb.predict_proba(X_test_bb)[:, 1]\n",
    "        except:\n",
    "            p_val_xgb = model_xgb.predict_proba(np.array(X_val_bb))[:, 1]\n",
    "            p_test_xgb = model_xgb.predict_proba(np.array(X_test_bb))[:, 1]\n",
    "    else:\n",
    "        print(\"![ERROR] Brak modelu XGBoost.\")\n",
    "        p_val_xgb, p_test_xgb = np.zeros(len(y_val)), np.zeros(len(y_test))\n",
    "\n",
    "    models_to_calibrate = [\n",
    "        (\"Logit\", p_val_logit, p_test_logit),\n",
    "        (\"XGBoost\", p_val_xgb, p_test_xgb)\n",
    "    ]\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    print(\">>> [4/6] Kalibracja...\")\n",
    "\n",
    "    for name, p_val, p_test in models_to_calibrate:\n",
    "        if np.sum(p_val) == 0: continue\n",
    "\n",
    "        print(f\"   ... Przetwarzanie: {name}\")\n",
    "        \n",
    "        # Definicja metod i predykcji\n",
    "        methods_map = {}\n",
    "        \n",
    "        # 0. Original\n",
    "        methods_map[\"Original\"] = p_test\n",
    "        results_table.append(compute_metrics(y_test, p_test, f\"{name}_Original\"))\n",
    "        \n",
    "        # 1. Platt\n",
    "        platt = LogisticRegression(C=99999, solver='lbfgs')\n",
    "        platt.fit(p_val.reshape(-1, 1), y_val)\n",
    "        p_test_platt = platt.predict_proba(p_test.reshape(-1, 1))[:, 1]\n",
    "        methods_map[\"Platt\"] = p_test_platt\n",
    "        results_table.append(compute_metrics(y_test, p_test_platt, f\"{name}_Platt\"))\n",
    "        \n",
    "        # 2. Isotonic\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(p_val, y_val)\n",
    "        p_test_iso = iso.predict(p_test)\n",
    "        methods_map[\"Isotonic\"] = p_test_iso\n",
    "        results_table.append(compute_metrics(y_test, p_test_iso, f\"{name}_Isotonic\"))\n",
    "        \n",
    "        # 3. Beta\n",
    "        beta = BetaCalibration()\n",
    "        beta.fit(p_val, y_val)\n",
    "        p_test_beta = beta.predict_proba(p_test)[:, 1]\n",
    "        methods_map[\"Beta\"] = p_test_beta\n",
    "        results_table.append(compute_metrics(y_test, p_test_beta, f\"{name}_Beta\"))\n",
    "        \n",
    "        # 4. Iso + Large 4%\n",
    "        p_val_iso = iso.predict(p_val)\n",
    "        cal_large = CalibrationInTheLarge(target_mean=TARGET_MEAN_PD)\n",
    "        cal_large.fit(p_val_iso)\n",
    "        p_test_final = cal_large.predict_proba(p_test_iso)[:, 1]\n",
    "        methods_map[\"Iso_Large4%\"] = p_test_final\n",
    "        results_table.append(compute_metrics(y_test, p_test_final, f\"{name}_Iso+Large4%\"))\n",
    "\n",
    "        # >>> GENEROWANIE ODDZIELNYCH WYKRES√ìW <<<\n",
    "        print(f\"       Generowanie wykres√≥w w {IMG_DIR}...\")\n",
    "        for method_name, prob_arr in methods_map.items():\n",
    "            # Bezpieczna nazwa pliku\n",
    "            safe_method = method_name.replace(\" \", \"\").replace(\"+\", \"_\").replace(\"%\", \"\")\n",
    "            \n",
    "            # 1. Reliability Curve\n",
    "            plot_single_reliability(\n",
    "                y_test, \n",
    "                prob_arr, \n",
    "                title=f\"Reliability: {name} - {method_name}\", \n",
    "                filename=f\"rel_{name}_{safe_method}.png\"\n",
    "            )\n",
    "            \n",
    "            # 2. Histogram\n",
    "            plot_single_histogram(\n",
    "                prob_arr, \n",
    "                title=f\"PD Hist: {name} - {method_name}\", \n",
    "                filename=f\"hist_{name}_{safe_method}.png\"\n",
    "            )\n",
    "\n",
    "    print(\">>> [5/6] Zapis tabeli wynik√≥w...\")\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "    cols = [\"Avg_PD\", \"ECE\", \"ACE\", \"Brier\", \"Rel\", \"Res\"]\n",
    "    for c in cols:\n",
    "        if c in df_results.columns:\n",
    "            df_results[c] = df_results[c].round(5)\n",
    "            \n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"wyniki_kalibracji.csv\")\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Wykresy (ka≈ºdy osobno) zapisano w: {IMG_DIR}\")\n",
    "    print(\">>> Zako≈Ñczono.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f693a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratingi.py\n",
    "\n",
    "\"\"\"\n",
    "Pipeline do:\n",
    "- wczytania modeli (logit WoE + XGBoost),\n",
    "- policzenia PD na train/val/test,\n",
    "- zbudowania rating√≥w (AAA...CCC) na podstawie PD,\n",
    "- wygenerowania tabel ratingowych i tabel decyzyjnych.\n",
    "\n",
    "Zak≈Çadamy, ≈ºe wej≈õciowe modele zwracajƒÖ ju≈º \"PD\" (docelowo: skalibrowane).\n",
    "Na razie mo≈ºna u≈ºywaƒá PD z niekalibrowanego logitu.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================================================\n",
    "#                   KONFIGURACJA ≈öCIE≈ªEK\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))        # .../IWUM-Projekt-1/Ratingi\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, \"..\")) # .../IWUM-Projekt-1\n",
    "\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"zbi√≥r_7.csv\")\n",
    "\n",
    "# interpretowalny logit + jego preproc WoE\n",
    "LOGIT_MODEL_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"Modele_interpretowalne\",\n",
    "    \"models\",\n",
    "    \"best_logistic_regression_woe.pkl\",\n",
    ")\n",
    "LOGIT_PREPROC_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"EDA\",\n",
    "    \"preprocesing_pipelines\",\n",
    "    \"preprocessing_logit_woe.pkl\",\n",
    ")\n",
    "\n",
    "# black-box XGBoost (tu przyjmujƒô strukturƒô podobnƒÖ jak w repo)\n",
    "XGB_MODEL_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"Modele_nieinterpretowalne\",\n",
    "    \"models_blackbox\",\n",
    "    \"best_xgboost.pkl\",\n",
    ")\n",
    "# TODO: je≈õli XGBoost ma sw√≥j pipeline/preproc, dodaj tu ≈õcie≈ºkƒô:\n",
    "XGB_PREPROC_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"EDA\",\n",
    "    \"preprocesing_pipelines\",\n",
    "    \"preprocessing_blackbox.pkl\",  \n",
    ")\n",
    "\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"rating_results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Nazwy rating√≥w ‚Äì rosnƒÖce ryzyko (AAA = najlepszy, CCC = najgorszy)\n",
    "RATING_LABELS = [\"AAA\", \"AA\", \"A\", \"BBB\", \"BB\", \"B\", \"CCC\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                     Wczytanie danych\n",
    "# ============================================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Wczytuje pe≈Çny zbi√≥r i robi podzia≈Ç 60/20/20 (train/val/test),\n",
    "    sp√≥jny z resztƒÖ projektu.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    y = df[\"default\"]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    ≈Åaduje:\n",
    "    - logit interpretowalny + pipeline WoE\n",
    "    - XGBoost + pipeline \n",
    "    \"\"\"\n",
    "    # logit\n",
    "    logit_model = joblib.load(LOGIT_MODEL_PATH)\n",
    "    logit_preproc = joblib.load(LOGIT_PREPROC_PATH)\n",
    "\n",
    "    # XGBoost \n",
    "    if os.path.exists(XGB_MODEL_PATH):\n",
    "        xgb_model = joblib.load(XGB_MODEL_PATH)\n",
    "    else:\n",
    "        xgb_model = None\n",
    "\n",
    "    if os.path.exists(XGB_PREPROC_PATH):\n",
    "        xgb_preproc = joblib.load(XGB_PREPROC_PATH)\n",
    "    else:\n",
    "        xgb_preproc = None\n",
    "\n",
    "    return logit_model, logit_preproc, xgb_model, xgb_preproc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                Predykcja PD dla modeli\n",
    "# ============================================================\n",
    "\n",
    "def predict_pd_logit(logit_model, logit_preproc, X):\n",
    "    \"\"\"\n",
    "    Zwraca przewidywane PD dla logitu.\n",
    "    \"\"\"\n",
    "    X_tr = logit_preproc.transform(X)\n",
    "    pd_hat = logit_model.predict_proba(X_tr)[:, 1]\n",
    "    return pd_hat\n",
    "\n",
    "\n",
    "def predict_pd_xgb(xgb_model, xgb_preproc, X):\n",
    "    \"\"\"\n",
    "    Zwraca przewidywane PD dla XGBoost.\n",
    "    \"\"\"\n",
    "    if xgb_model is None:\n",
    "        return None\n",
    "\n",
    "    if xgb_preproc is not None:\n",
    "        X_tr = xgb_preproc.transform(X)\n",
    "    else:\n",
    "        X_tr = X\n",
    "\n",
    "    if hasattr(xgb_model, \"predict_proba\"):\n",
    "        pd_hat = xgb_model.predict_proba(X_tr)[:, 1]\n",
    "    else:\n",
    "        # niekt√≥re implementacje zwracajƒÖ bezpo≈õrednio PD\n",
    "        pd_hat = xgb_model.predict(X_tr)\n",
    "    return pd_hat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#           Budowa rating√≥w na podstawie PD\n",
    "# ============================================================\n",
    "\n",
    "def build_rating_bins_by_quantiles(pd_train, n_classes=7):\n",
    "    \"\"\"\n",
    "    Wyznacza progi rating√≥w na podstawie kwantyli PD z TRAIN.\n",
    "\n",
    "    Zwraca tablicƒô krawƒôdzi [b0, b1, ..., b_n], gdzie:\n",
    "    - b0 = 0.0\n",
    "    - b_n = 1.0\n",
    "    \"\"\"\n",
    "    quantiles = np.linspace(0, 1, n_classes + 1)\n",
    "    bin_edges = np.quantile(pd_train, quantiles)\n",
    "\n",
    "    # upewniamy siƒô, ≈ºe zakres jest ca≈Çy [0,1]\n",
    "    bin_edges[0] = 0.0\n",
    "    bin_edges[-1] = 1.0\n",
    "\n",
    "    # ma≈Çe zabezpieczenie przed duplikatami prog√≥w\n",
    "    bin_edges = np.unique(bin_edges)\n",
    "    if len(bin_edges) - 1 < n_classes:\n",
    "        # je≈õli duplikaty, mamy mniej \"slot√≥w\" ratingowych,\n",
    "        # wiƒôc skracamy listƒô RATING_LABELS przy mapowaniu\n",
    "        print(\"‚ö†Ô∏è Ostrze≈ºenie: duplikujƒÖce siƒô progi rating√≥w (ma≈Ço zr√≥≈ºnicowane PD).\")\n",
    "    return bin_edges\n",
    "\n",
    "\n",
    "def assign_ratings(pd_hat, bin_edges, labels):\n",
    "    \"\"\"\n",
    "    Przypisuje ratingi na podstawie PD i prog√≥w.\n",
    "\n",
    "    pd_hat   : wektor PD\n",
    "    bin_edges: krawƒôdzie przedia≈Ç√≥w (rosnƒÖce)\n",
    "    labels   : list[str], np. [\"AAA\", \"AA\", ..., \"CCC\"]\n",
    "\n",
    "    Zwraca Series dtype=category.\n",
    "    \"\"\"\n",
    "    # je≈õli z powodu duplikat√≥w prog√≥w mamy mniej przedzia≈Ç√≥w\n",
    "    n_intervals = len(bin_edges) - 1\n",
    "    if n_intervals != len(labels):\n",
    "        labels = labels[:n_intervals]\n",
    "\n",
    "    ratings = pd.cut(\n",
    "        pd_hat,\n",
    "        bins=bin_edges,\n",
    "        labels=labels,\n",
    "        right=False,   # lewostronnie domkniƒôte: [b_i, b_{i+1})\n",
    "        include_lowest=True,\n",
    "    )\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def rating_summary(y_true, pd_hat, ratings, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Podsumowanie rating√≥w:\n",
    "    - liczebno≈õƒá\n",
    "    - liczba bad\n",
    "    - bad rate\n",
    "    - ≈õrednie PD\n",
    "\n",
    "    Zwraca DataFrame + wypisuje na ekran.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"y\": y_true,\n",
    "        \"pd\": pd_hat,\n",
    "        \"rating\": ratings,\n",
    "    })\n",
    "\n",
    "    summary = (\n",
    "        df.groupby(\"rating\")\n",
    "          .agg(\n",
    "              n_obs=(\"y\", \"size\"),\n",
    "              n_bad=(\"y\", \"sum\"),\n",
    "              bad_rate=(\"y\", \"mean\"),\n",
    "              avg_pd=(\"pd\", \"mean\"),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RATING SUMMARY ‚Äì {model_name} ‚Äì {dataset_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(summary.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#          Funkcje do prog√≥w decyzyjnych / tabel decyzyjnych\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "profit_good_accepted=0.15   # +15% na dobrym kredycie\n",
    "loss_bad_accepted=0.5      # -50% na z≈Çym kredycie\n",
    "cost_reject_good= 0.06     # utrata ~40% potencjalnego zysku\n",
    "profit_reject_bad=0.2      # unikniƒôcie 40% potencjalnej straty\n",
    "\n",
    "\n",
    "def expected_profit(\n",
    "    y_true,\n",
    "    pd_hat,\n",
    "    threshold,\n",
    "    profit_good_accepted=0.15,   # +15% na dobrym kredycie\n",
    "    loss_bad_accepted=0.50,      # -50% na z≈Çym kredycie\n",
    "    frac_aux=0.4                 # u≈Çamek dla utraconego zysku / unikniƒôtej straty\n",
    "):\n",
    "    \"\"\"\n",
    "    Liczy oczekiwany zysk portfela dla danego progu PD.\n",
    "\n",
    "    Znaczenie:\n",
    "    - y_true = 0 -> dobry klient\n",
    "    - y_true = 1 -> z≈Çy klient (default)\n",
    "    - akceptujemy je≈õli PD <= threshold\n",
    "\n",
    "    Przypadki:\n",
    "    - good & accepted   -> +profit_good_accepted\n",
    "    - bad  & accepted   -> -loss_bad_accepted\n",
    "    - good & rejected   -> cost_reject_good  (ujemny)\n",
    "    - bad  & rejected   -> profit_reject_bad (dodatni)\n",
    "    \"\"\"\n",
    "\n",
    "    cost_reject_good = -frac_aux * profit_good_accepted   # np. -0.06\n",
    "    profit_reject_bad = frac_aux * loss_bad_accepted      # np. +0.20\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    pd_hat = np.asarray(pd_hat)\n",
    "\n",
    "    accept = pd_hat <= threshold\n",
    "    reject = ~accept\n",
    "\n",
    "    good = (y_true == 0)\n",
    "    bad  = (y_true == 1)\n",
    "\n",
    "    n_A_good = np.sum(accept & good)\n",
    "    n_A_bad  = np.sum(accept & bad)\n",
    "    n_R_good = np.sum(reject & good)\n",
    "    n_R_bad  = np.sum(reject & bad)\n",
    "\n",
    "    return (\n",
    "        n_A_good * profit_good_accepted\n",
    "        - n_A_bad  * loss_bad_accepted\n",
    "        + n_R_bad  * profit_reject_bad\n",
    "        + n_R_good * cost_reject_good\n",
    "    )\n",
    "\n",
    "\n",
    "def decision_table(y_true, pd_hat, thresholds):\n",
    "    \"\"\"\n",
    "    Buduje tabelƒô decyzyjnƒÖ dla r√≥≈ºnych prog√≥w PD:\n",
    "    - udzia≈Ç zaakceptowanych / odrzuconych\n",
    "    - bad rate w portfelu zaakceptowanym / odrzuconym\n",
    "    - liczby TP, FP, FN, TN\n",
    "\n",
    "    Zwraca DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    n = len(y_true)\n",
    "\n",
    "    for thr in thresholds:\n",
    "        y_pred = (pd_hat <= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        accepted = tp + fp\n",
    "        rejected = tn + fn\n",
    "\n",
    "        row = {\n",
    "            \"threshold\": thr,\n",
    "            \"accept_rate\": accepted / n,\n",
    "            \"reject_rate\": rejected / n,\n",
    "            \"bad_rate_accepted\": fp / accepted if accepted > 0 else np.nan,\n",
    "            \"bad_rate_rejected\": fn / rejected if rejected > 0 else np.nan,\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn,\n",
    "            \"TN\": tn,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def build_cost_curve(\n",
    "    y_true,\n",
    "    pd_hat,\n",
    "    thresholds,\n",
    "    model_name=\"Model\",\n",
    "    save_path=None,\n",
    "    profit_good_accepted=0.15,\n",
    "    loss_bad_accepted=0.50,\n",
    "    frac_aux=0.4\n",
    "):\n",
    "    \"\"\"\n",
    "    Buduje cost curve: pr√≥g PD -> oczekiwany zysk.\n",
    "    \"\"\"\n",
    "\n",
    "    profits = []\n",
    "    for thr in thresholds:\n",
    "        prof = expected_profit(\n",
    "            y_true,\n",
    "            pd_hat,\n",
    "            thr,\n",
    "            profit_good_accepted=profit_good_accepted,\n",
    "            loss_bad_accepted=loss_bad_accepted,\n",
    "            frac_aux=frac_aux,\n",
    "        )\n",
    "        profits.append(prof)\n",
    "\n",
    "    curve_df = pd.DataFrame({\n",
    "        \"threshold\": thresholds,\n",
    "        \"expected_profit\": profits,\n",
    "    })\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.figure()\n",
    "        plt.plot(thresholds, profits, marker=\"o\")\n",
    "        plt.xlabel(\"Pr√≥g PD (akceptujemy je≈õli PD ‚â§ pr√≥g)\")\n",
    "        plt.ylabel(\"Oczekiwany zysk (jednostki umowne)\")\n",
    "        plt.title(f\"Cost curve ‚Äì {model_name}\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    return curve_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                           MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # 1. Dane\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n",
    "\n",
    "    # 2. Modele\n",
    "    logit_model, logit_preproc, xgb_model, xgb_preproc = load_models()\n",
    "\n",
    "    # 3. PD z logitu (tu docelowo mo≈ºesz wstawiƒá PD po kalibracji)\n",
    "    pd_train_logit = predict_pd_logit(logit_model, logit_preproc, X_train)\n",
    "    pd_val_logit   = predict_pd_logit(logit_model, logit_preproc, X_val)\n",
    "    pd_test_logit  = predict_pd_logit(logit_model, logit_preproc, X_test)\n",
    "\n",
    "    # 4. (opcjonalnie) PD z XGBoost\n",
    "    if xgb_model is not None:\n",
    "        pd_train_xgb = predict_pd_xgb(xgb_model, xgb_preproc, X_train)\n",
    "        pd_val_xgb   = predict_pd_xgb(xgb_model, xgb_preproc, X_val)\n",
    "        pd_test_xgb  = predict_pd_xgb(xgb_model, xgb_preproc, X_test)\n",
    "    else:\n",
    "        pd_train_xgb = pd_val_xgb = pd_test_xgb = None\n",
    "\n",
    "    # 5. Budowa prog√≥w ratingowych na podstawie PD z TRAIN (logit)\n",
    "    bin_edges = build_rating_bins_by_quantiles(\n",
    "        pd_train_logit,\n",
    "        n_classes=len(RATING_LABELS)\n",
    "    )\n",
    "\n",
    "    # 6. Przypisanie rating√≥w dla logitu\n",
    "    ratings_train_logit = assign_ratings(pd_train_logit, bin_edges, RATING_LABELS)\n",
    "    ratings_val_logit   = assign_ratings(pd_val_logit,   bin_edges, RATING_LABELS)\n",
    "    ratings_test_logit  = assign_ratings(pd_test_logit,  bin_edges, RATING_LABELS)\n",
    "\n",
    "    # 7. Podsumowania ratingowe (logit)\n",
    "    summary_train_logit = rating_summary(\n",
    "        y_train, pd_train_logit, ratings_train_logit,\n",
    "        model_name=\"Logit_WoE\",\n",
    "        dataset_name=\"TRAIN\",\n",
    "    )\n",
    "    summary_val_logit = rating_summary(\n",
    "        y_val, pd_val_logit, ratings_val_logit,\n",
    "        model_name=\"Logit_WoE\",\n",
    "        dataset_name=\"VAL\",\n",
    "    )\n",
    "    summary_test_logit = rating_summary(\n",
    "        y_test, pd_test_logit, ratings_test_logit,\n",
    "        model_name=\"Logit_WoE\",\n",
    "        dataset_name=\"TEST\",\n",
    "    )\n",
    "\n",
    "    # 8. (opcjonalnie) te same ratingi dla XGBoost ‚Äì u≈ºywamy TYCH SAMYCH prog√≥w PD\n",
    "    if pd_train_xgb is not None:\n",
    "        ratings_train_xgb = assign_ratings(pd_train_xgb, bin_edges, RATING_LABELS)\n",
    "        ratings_val_xgb   = assign_ratings(pd_val_xgb,   bin_edges, RATING_LABELS)\n",
    "        ratings_test_xgb  = assign_ratings(pd_test_xgb,  bin_edges, RATING_LABELS)\n",
    "\n",
    "        summary_train_xgb = rating_summary(\n",
    "            y_train, pd_train_xgb, ratings_train_xgb,\n",
    "            model_name=\"XGBoost\",\n",
    "            dataset_name=\"TRAIN\",\n",
    "        )\n",
    "        summary_val_xgb = rating_summary(\n",
    "            y_val, pd_val_xgb, ratings_val_xgb,\n",
    "            model_name=\"XGBoost\",\n",
    "            dataset_name=\"VAL\",\n",
    "        )\n",
    "        summary_test_xgb = rating_summary(\n",
    "            y_test, pd_test_xgb, ratings_test_xgb,\n",
    "            model_name=\"XGBoost\",\n",
    "            dataset_name=\"TEST\",\n",
    "        )\n",
    "    else:\n",
    "        summary_train_xgb = summary_val_xgb = summary_test_xgb = None\n",
    "\n",
    "    # 9. Tabele decyzyjne dla logitu (np. na WALIDACJI)\n",
    "    thresholds = np.linspace(0.02, 0.98, 50)  # zakres PD do analizy\n",
    "    decision_val_logit = decision_table(y_val, pd_val_logit, thresholds)\n",
    "    decision_test_logit = decision_table(y_test, pd_test_logit, thresholds)\n",
    "\n",
    "    print(\"\\nDECISION TABLE ‚Äì Logit ‚Äì VAL\")\n",
    "    print(decision_val_logit.to_string(index=False))\n",
    "\n",
    "    print(\"\\nDECISION TABLE ‚Äì Logit ‚Äì TEST\")\n",
    "    print(decision_test_logit.to_string(index=False))\n",
    "\n",
    "     # 9b. Tabele decyzyjne dla XGBoost (je≈õli model istnieje)\n",
    "    if pd_val_xgb is not None:\n",
    "        decision_val_xgb = decision_table(y_val, pd_val_xgb, thresholds)\n",
    "        decision_test_xgb = decision_table(y_test, pd_test_xgb, thresholds)\n",
    "\n",
    "        print(\"\\nDECISION TABLE ‚Äì XGBoost ‚Äì VAL\")\n",
    "        print(decision_val_xgb.to_string(index=False))\n",
    "\n",
    "        print(\"\\nDECISION TABLE ‚Äì XGBoost ‚Äì TEST\")\n",
    "        print(decision_test_xgb.to_string(index=False))\n",
    "    else:\n",
    "        decision_val_xgb = decision_test_xgb = None\n",
    "\n",
    "    # 9c. Cost curves ‚Äì logit\n",
    "    cost_curve_val_logit = build_cost_curve(\n",
    "        y_val, pd_val_logit, thresholds,\n",
    "        model_name=\"Logit_WoE\",\n",
    "        save_path=os.path.join(RESULTS_DIR, \"cost_curve_logit_val.png\"),\n",
    "    )\n",
    "    cost_curve_test_logit = build_cost_curve(\n",
    "        y_test, pd_test_logit, thresholds,\n",
    "        model_name=\"Logit_WoE\",\n",
    "        save_path=os.path.join(RESULTS_DIR, \"cost_curve_logit_test.png\"),\n",
    "    )\n",
    "\n",
    "    # Cost curves ‚Äì XGBoost (je≈õli jest)\n",
    "    if pd_val_xgb is not None:\n",
    "        cost_curve_val_xgb = build_cost_curve(\n",
    "            y_val, pd_val_xgb, thresholds,\n",
    "            model_name=\"XGBoost\",\n",
    "            save_path=os.path.join(RESULTS_DIR, \"cost_curve_xgb_val.png\"),\n",
    "        )\n",
    "        cost_curve_test_xgb = build_cost_curve(\n",
    "            y_test, pd_test_xgb, thresholds,\n",
    "            model_name=\"XGBoost\",\n",
    "            save_path=os.path.join(RESULTS_DIR, \"cost_curve_xgb_test.png\"),\n",
    "        )\n",
    "\n",
    "    # 10. Zapis wynik√≥w do CSV (≈ºeby mo≈ºna by≈Ço wciƒÖgnƒÖƒá do raportu / Excela)\n",
    "    summary_train_logit.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"rating_summary_logit_train.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    summary_val_logit.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"rating_summary_logit_val.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    summary_test_logit.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"rating_summary_logit_test.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    decision_val_logit.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"decision_table_logit_val.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    decision_test_logit.to_csv(\n",
    "        os.path.join(RESULTS_DIR, \"decision_table_logit_test.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    if summary_train_xgb is not None:\n",
    "        summary_train_xgb.to_csv(\n",
    "            os.path.join(RESULTS_DIR, \"rating_summary_xgb_train.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        summary_val_xgb.to_csv(\n",
    "            os.path.join(RESULTS_DIR, \"rating_summary_xgb_val.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        summary_test_xgb.to_csv(\n",
    "            os.path.join(RESULTS_DIR, \"rating_summary_xgb_test.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        if decision_val_xgb is not None:\n",
    "            decision_val_xgb.to_csv(\n",
    "                os.path.join(RESULTS_DIR, \"decision_table_xgb_val.csv\"),\n",
    "                index=False,\n",
    "            )\n",
    "            decision_test_xgb.to_csv(\n",
    "                os.path.join(RESULTS_DIR, \"decision_table_xgb_test.csv\"),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "    print(\"\\nZapisano tabele ratingowe i decyzyjne do:\", RESULTS_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
